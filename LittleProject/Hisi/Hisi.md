

[TOC]

------




# 第零章 海思方案全面介绍

## 0.0 章节概要

**1.为何要做海思HI3518E方案完全学习项目**

  本节介绍本项目课程的时代背景和价值，让大家学习之前先了解自己所学的价值再动手。

**2.海思方案项目用到的硬件平台介绍**

  本节介绍专为项目课程所定制的硬件开发板平台和细节配置信息。

**3.课程规划和核心技术介绍**

  本节介绍整个项目课程的完整规划，和其中涉及到的核心技术。

**4.学习套装规划和学习周期费用等问题**

  本节介绍学习套装的设定，各自适合的群体，项目的学习周期学习费用等杂项问题。

## 0.1.为何要做海思HI3518E方案完全学习项目

### 0.1.1、视频是一个方兴未艾的大产业

(1)人类社会信息产品发展趋势：物理文本->电子文本->图片->音视频。

(2)音视频行业方兴未艾：优酷&爱奇艺等、美拍&各种小视频、视频监控、人脸识别&自动驾驶。

(3)科技以人为本，视频符合人类的天然需求，必然要得到大发展。

(4)技术融合是趋势：互联网、人工智能、视频采集和处理等技术会融合，构成奇幻美妙的现代科技生活。

### 0.1.2、视频行业潜在商机大、人才缺口大

(1)视频监控、手机拍照等“传统”视频领域仍有巨大发展空间。

(2)自动驾驶、人脸识别闸机、刷脸支付等，潜在商机巨大，市场需求尚未井喷。

(3)视频行业专业性强、学习周期长、技术难度大，技术门槛和壁垒效应明显。

(4)行业出现时间短，发展快，根本没有相对应的课程体系和专门学习资料，学习无处下手。

 

(5)中国处于这一波技术革命的浪潮中心，产业配套最全、综合实力最强、产品出货量最大，所以人才需求最旺盛。但同时中国技术积累和底蕴薄弱，高端人才严重匮乏。

### 0.1.3、为什么选择海思HI3518E方案来讲

(1)海思的前身是华为的半导体部门，主要产品线包括：智能手机处理器麒麟系列，视频采集和编解码处理器HI35XX系列，无线通信方向芯片等。

(2)早年（2013、2014年之前）视频行业主要被TI等美国公司控制，台湾地区发展也很

快，海思进入后凭借华为的技术和管理能力，迅速发展壮大，在高性价比消费级视频领域已经独霸了。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        

(3)海思还在不断发展，逐步渗透汽车级（行车记录仪、ADAS、自动驾驶等）、运动级（航拍、运动DV等）领域。

(4)还有更高端的工业级（工业相机譬如缺陷检测、机器视觉等）和军工级等多被美欧日德等把持，等待我们去征伐。

(5)一句话：海思方案用量大、范围广、工资高、潜力大，不学这个学什么？

## 0.2.海思方案项目用到的硬件平台介绍

### 0.2.1、视频实时预览演示

### 0.2.2、实验箱图片浏览和配置讲解

(1)处理器：   HI3518E

(2)内存：   内置64MB DDR内存

(3)Flash:     板载16MB SPIFlash

(4)SENSOR：   OV9712 + AR0130或其他

(5)有线网络： 板载支持

(6)调试串口： 1路TTL串口

(7)底层调试： 预留JTAG口

(8)预留按键： 3个

(9)扩展存储:  TF卡

 

(10)WIFI扩展：  USB WIFI

**注****：SPI flash**

　　SPI一种通信接口。那么严格的来说SPI Flash是一种使用SPI通信的Flash，即，可能指NOR也可能是NAND。但现在大部分情况默认下人们说的SPI Flash指的是SPI NorFlash。早期Norflash的接口是parallel的形式，即把数据线和地址线并排与IC的管脚连接。但是后来发现不同容量的Norflash不能硬件上兼容（数据线和地址线的数量不一样），并且封装比较大，占用了较大的PCB板位置，所以后来逐渐被SPI（串行接口）Norflash所取代。同时不同容量的SPI Norflash管脚也兼容封装也更小。，至于现在很多人说起NOR flash直接都以SPI flash来代称。

### 0.2.3、处理器为什么选HI3518E

(1)HI35XX系列有很多型号，其中HI3518E是入门级最高性价比的型号，出货量很大。

(2)不同型号的差异主要是支持视频像素数和分辨率、通道数、高端特性差异等。

(3)不同型号的相同点：同一套思路的海思SDK、视频行业基础概念和经验、H.264编解码算法和编程技术、RTSP/ORTP等网络传输、P2P等外网视频直播技术等。

(4)综合比较结果：HI3518E最适合入门学习，硬件本身特性三年行业经验，年薪20-30万以内不会妨碍。

 

 

## 0.3.课程规划和核心技术介绍

### 0.3.1、完全学习分11季课程，详见最下面附录课程介绍

**第1季：全面认识海思SDK及嵌入式层开发**

  本季首先介绍开发板整个硬件配置和测试方法，然后围绕海思官方SDK开发包讲解各种官方资料的用法，最后用大量时间讲解如何在ubuntu中搭建海思视频方案的开发环境，如uboot的配置和编译、烧写；kernel的配置和编译、rootfs的制作，tftp方式下载烧写并启动linux系统，rootfs中各种配置文件的含义、ko文件的加载、mpp库的部署、sample程序的编译和运行等。

**第2季：从官方例程深度学习海思SDK及API**

  本季课程详细分析海思SDK中的sample程序的实现，并顺带讲解视频采集中的很多基本概念，视频sensor的工作原理、像素和视频分辨率的关系、rawRGB和RGB888的换算方式等，本季课程的重点内容是海思SDK中API文档的带读，学习海思方案重点就在于学习这套API，这是海思方案视频采集和H.264编码的关键所在，因此本季课程非常关键，学完本季课程才表示对海思方案开发入门了。

**第3季：ORTP库局域网图传和VLC实时预览**

本季课程通过移植ORTP库并调用ORTP库函数来实现H.264裸流数据的网络传输，并且在Windows中安装VLC播放器，并在VLC中通过配置文件的方式来解析ORTP发送的裸流实现视频实时预览。本季课程的核心是ORTP协议的讲解，ORTP库的移植、部署和调用，VLC软件的用法讲解和配置文件讲解，Wireshark网络抓包工具的用法讲解，实时视频流传输和解析的体验和编程实现。

**第4季：图像sensor的特性和驱动解析**

  本季课程以配置的2款图像sensor为案例来讲解图像sensor的关键特性和相关数据结构，I2C控制接口和图像数据接口，海思SDK中sensor的驱动框架和编译、部署、加载，sensor调试的一般方法和步骤等。

**第5季：海思平台上USB WIFI的移植与局域网无线调试和视频流预览**

  本季课程在开发板上移植USB WIFI模块（套装附带的MT7601模块），工作在ap模式下，并且用电脑通过WIFI去连接开发板实现主机和开发板的无线网络局域网通信，这样就可以用WIFI网络取代板载ethernet有线网络方式来进行调试和局域网视频预览。现在很多的视频摄像头产品都要求支持无线网络连接功能，本季课程主要是针对这方面开发技术讲解的。

**第6季：RTSP协议详解与实时流视频预览**

  本季详细讲解RTSP协议的技术细节，并且编程实现基于RTSP协议的实时视频流传输，在局域网内浏览实时监控画面。RTSP是实时视频网络传输最主流的实现方式，低延时高清晰度的RTSP视频流传输是网络直播、在线会议系统等行业的核心技术，本季课程会详细讲解相关概念、编程实战和调试技巧等。

**第7季：视频打包为MP4格式并存储到TF卡的实现**

  本季实现将编码后的H.264视频流打包为MP4格式的录像，并存储到板载TF卡中。这个过程涉及到的一个主要技术是H.264的视频帧格式，I帧、P帧等概念，另一个主要技术是MP4格式的文件头、封包技术、MP4打包库的移植部署和调用等，最后是linux下TF卡的存储和文件管理（使用了FAT32文件系统）。编码后图像打包成MP4存储的功能在行车记录仪、监控摄像头等产品上均是刚需功能，尤其是行车记录仪，学好该季课程去做这类产品开发有非常大优势。

**第8季：海思平台OSD的实现**

  OSD（on screen display）功能应用很广泛，譬如监控视频中的实时时间显示、电视转播中的台标和字幕等。本季讲解如何在海思平台上实现图像OSD，核心是海思SDK中提供的OSD功能相关的一些API的使用和调试技巧。

**第9季：图像的IQ调试**

  sensor直接采集的图像都有各种不理想性，因此在编码前都会经过一个软件方式处理，这个处理就叫ISP，图像的IQ调试就是研究这些处理算法和实现的。常见的IQ调试技术如：线性纠正、噪声去除、黑电平校正、坏点去除、颜色插补、Gamma 校正、RGB2YUV 转换、主动白平衡处理、主动曝光控制、AE评估等。

**第10季：P2P方式实现视频远程直播**

  本季课程主要讲解如何实现视频的外网传输，核心技术是P2P内网穿透的实现和视频转发服务器的架构。虽然也可以自己搭建视频服务器，但是实际开发产品时经常采用的是一些第三方的视频服务，本季课程中两种开发方式都会涉及到，并力求让大家掌握其实现原理和编程技术、调试技术。

**第11季：基于opencv的图像识别开发**

  本季课程属于扩展开发，讲解如何在主机ubuntu中搭建和部署opencv的环境，将HI3518E采集并编码后的视频传输到主机中并且使用opencv来进行图像识别和处理。近年来随着人工智能技术的火爆，图像识别和视频识别技术越来越被看重，人脸识别闸机、刷脸支付、自动驾驶汽车、ADAS系统等应用场景均有赖于视频识别的技术突破。本季课程算是抛砖引玉，向大家引入图像识别技术的开发基础、环境搭建、基本算法原理、简单应用案例，希望帮助大家进入该技术领域。

 

## 0.4.学习套装规划和学习周期费用等问题

### 0.4.1、学习套装介绍，详见项目描述文档

大侠出道套装：1-3季

大牛养成套装：1-8季

大神之路套装：1-11季

 

### 0.4.2、项目适合哪些人

### 0.4.3、参加学习的条件

### 0.4.4、学完本项目能得到什么

### 0.4.5、学习方式和学习时长

### 0.4.6、核心技术团队介绍

 

 

 

 

 

 

 

 

 

 

 

 

 



 

# 第一章 全面认识海思SDK及嵌入式层开发

## 1.0 章节概要

**1.1.全面认识和检测配套开发套装1**

  本节全面细致的介绍开发板的组合和配置，并且讲解如何搭建测试程序的环境。

**1.2.全面认识和检测配套开发套装2**

  本节接上节完成开发板软硬件测试的流程，用我们提供的sample程序可以通过rtsp实时预览图像信息，则证明开发板硬件完好。

**1.3.视频设备开发的技术流**

  本节从理论框架上讲解视频设备开发的整个产业链，让大家从宏观上把控这个行业涉及到的技术和细分市场。

**1.4.HI3518E方案系统整体架构介绍**

  本节从硬件架构和软件架构两个角度详细讲了HI3518E方案的构成，这个可以让大家从技术角度对海思这套体系有个轮廓性认知。

**1.5.海思SDK的整体介绍**

  本节介绍海思官方软件开发包的整个目录结构与内容，其中有些一带而过，有些就值得关注，多花些时间来看。

**1.6.海思SDK包的学习和实验1**

  本节将SDK压缩包在linux中解压，并且研究整个SDK的顶层设计，官方提供的脚本等。

**1.7.海思SDK包的学习和实验2**

  本节利用SDK中提供的脚本来整体处理SDK软件包，其实人家都设计好了，我们只需要学习并且利用好这套机制就OK了。

**1.8.在ubuntu16.0403X64上安装海思交叉编译工具链**

  本节讲海思工具链的安装，折腾了很多后发现还是要用SDK中提供的安装脚本最合适。

**1.9.编译osdrv**

  本节开始编译osdrv，这里面包含了uboot、kernel和rootfs全部。

**1.10.编译rootfs1**

  本节解决编译中遇到的问题，uboot和kernel总体来说非常顺利，主要是rootfs编译会遇到一些细节问题。

**1.11.编译rootfs2**

  本节解决了所有的问题，并且对Makefile进行修改跳过了一些问题，最终编译完了rootfs

**1.12.uboot的烧写和flash分区1**

  本节花了大量时间讲解一般SoC的裸机烧录方法，这些内容其实很普遍，你以后接触其他家的SoC时也会用得到。

**1.13.uboot的烧写和flash分区2**

  本节讲解HiTool工具的使用，并且使用HiTool来烧写uboot，并且讲了对SPIFlash的分区，下节烧录时会用到这些分区。

**1.14.kernel和rootfs烧录与启动系统**

  本节讲了kernel和rootfs的烧录，并且设置了正确的bootcmd和bootargs来启动linux内核。

**1.15.rootfs启动后做了什么**

  本节主要分析海思默认的rootfs中启动后做了哪些操作，这些主要在/etc目录下体现。

**1.16.mpp的部署研究和实战1**

  本节正式介绍mpp的各个文件夹，其中最关键是ko和lib，这两个是我们部署mpp的主要工作，sample程序运行需要这些ko和lib。

**1.17.mpp的部署研究和实战2**

  本节进行部署实战，在开发板中已经烧录的默认rootfs中增加部署ko和lib，并且修改profile文件，运行load3518e脚本。

**1.18.sample的编译和测试**

  本节编译SDK中mpp自带的sample，并且丢到开发板中去运行，测试结果ok，第一季正式圆满结束。

## 1.1_2.全面认识和检测配套开发套装1_2

### 1.1.1、套装配件介绍 

(1)主板

(2)默认安装SENSOR组合：AR0130

(3)备用SENSOR组合2：OV9712

(4)隔离式USB转串口小板+杜邦线

 

(5)USB WIFI网卡

(6)DC供电线

(7)网线

### 1.1.2、检测开发板

(1)出厂默认烧录了系统，但是没有部署应用程序，所以测试略麻烦

(2)要测试先接线：USB转串口、网线、电源线三个缺一不可

(3)虚拟机要布置好然后开启，用我给的或者按我的教程自己装一个，注意虚拟机的网络设置有问题导致ens33不能用，要改一下

(4)SecureCRT监视打开，开发板开机，自动挂载到/home/aston/rootfs中

(5)自动加入/mnt中，执行./sample_venc即可

(6)在vlc中输入地址，即可浏览实时画面

菜单栏：媒体->打开网络串流->网络，输入：rtsp://192.168.1.10:554/stream_chn0.h264

勾选：显示更多选项。在正在缓冲中设置为300（原来是1000）

### 1.1.3、注意

(1)镜头可以自己拧，一边拧一边看图像清晰度，调整到最合适，然后固定螺丝即可。注意有延时的，所以拧一下要停一下看效果。而且延时会累计，所以延时会越来越大。延时如果太大，把VLC关闭重新打开一次就ok了。

(2)图像的4个角可能会有黑边，原因是镜头没有装载正中间，镜头螺丝可以重新固定一下。

(3)图像是有畸变的，正常的，镜头的原因。

(4)有些同学的OV9712是有瑕疵的，下次生产会补发。 

 

 

## 1.3.视频设备开发的技术流

### 1.3.1、视频从产生到被消费的整个流程

(1)基本认知：视频是由单帧图像以每秒x帧的速率连续组成的，单帧图像类似位图。

(2)原始视频产生：镜头和sensor

(3)图像处理：ISP（image signal processing）

(4)视频编码压缩：h.264/h.265压缩算法，运算，内置DSP进行压缩运算的。

(5)视频流传输：网络传输、http/rtsp等

(6)视频存储：打包成MP4等格式存储，等待调阅

(7)视频回放：解码+播放

注：DSP（digital signal processor）是一种独特的微处理器，是以数字信号来处理大量信息的器件。其工作原理是接收模拟信号，转换为0或1的数字信号，再对数字信号进行修改、删除、强化，并在其他系统芯片中把数字数据解译回模拟数据或实际环境格式。它不仅具有可编程性，而且其实时运行速度可达每秒数以千万条复杂指令程序，远远超过通用微处理器，是数字化电子世界中日益重要的电脑芯片。它的强大数据处理能力和高运行速度，是最值得称道的两大特色。

### 1.3.2、视频行业的商业角度分段

(1)主芯片商、sensor、镜头等分立原件厂商

(2)模组厂商，类似大拿

(3)视频服务器厂商，类似大拿这种

(4)面向解决方案的方案开发商

(5)工程商或销售商

### 1.3.3、几个疑问点

(1)视频为什么要编码和解码？

  原始的视频数据太大了，根本没法传输。

(2)HI3518E主要解决什么问题？

  主要是编码压缩以及ISP；

(3)为什么使用linux而不是其他os？

   Linux网络协议栈是最全的。

## 1.4.HI3518E方案系统整体架构介绍

### 1.4.1、硬件上

(1)HI3518E单芯片提供：CPU+DSP+内置64MB DDR + ETHERNET MAC

(2)外置SPIFlash(16MB)用来存放程序（uboot、kernel、rootfs、app），SPI接口的Flash

(3)SDcard扩展提供用户数据区,用来存我们用来存录下来的视频的

(4)板载ethernet PHY和USB HOST扩展WIFI提供联网能力

(5)sensor接口（并行数据通道+I2C控制通道）提供主板和sensor链接

(6)串口作为调试口和linux系统控制台

### 1.4.2、软件上

(1)SPIFlash分区烧录uboot.bin、zImage、rootfs，并设置合理的环境变量使系统启动。

(2)sensor、ethernet等硬件均需要驱动支持

(3)app实现视频采集、编码压缩、网络传输等核心工作

(4)各种专业工作（譬如利用内置DSP实现h.264编码压缩）都由海思开发好并以ko的形式提供，有API文档参考，app编写者在sample的帮助下逐步实现自定义的功能。

## 1.5.海思SDK的整体介绍

 

 

 

 

 

 

 

 

## 1.6.海思SDK包的学习和实验1

### 1.6.1、2篇相关文档

### 1.6.2、SDK包复制到linux原生目录中并解压

### 1.6.3、SDK包操作的脚本程序研究

Hi3518E_SDK_V1.0.3.0/sdk.cleanup ,sdk.unpack 分别为清理,解压脚本,他们对应的执行程序的函数在Hi3518E_SDK_V1.0.3.0/scripts/common.sh 这个脚本中.我们执行了解压脚本,就将packge中的压缩包个解压出来了。

 

## 1.7.海思SDK包的学习和实验2

### 1.7.1、SDK的清理和解压脚本浏览

  浏览Hi3518E_SDK_V1.0.3.0/osdrv/底下的readme_ch.text，里面记录了如何编译的命令。

本目录下的编译脚本支持选用下文提到的两种工具链中的任何一种进行编译，因此编译时需要带上一个编译参数以指定对应的工具链 -- arm-hisiv300-linux 和 arm-hisiv400-linux。其中，arm-hisiv300-linux工具链对应uclibc库，arm-hisiv400-linux工具链对应glibc库。

  因为我们使用的16MB的SPI Flash，glibc库文件太大，可能放不下，所以我们就使用功能没有glibc库丰富但是体积较小的uclibc库。

### 1.7.2、SDK中源码包部分的配置编译分解

编译第一步，清除整个osdrv目录的编译文件：

  make OSDRV_CROSS=arm-hisiv300-linux CHIP=hi3518ev200 clean

编译第二部，编译整个osdrv目录：

  make OSDRV_CROSS=arm-hisiv300-linux CHIP=hi3518ev200 all

遇到问题1： pushd not found

  解决：将/bin/sh ->dash 链接删除，指向 /bin/sh->bash

​      lin -s /bin/bash /bin/sh    

遇到问题2：make[1]: arm-hisiv300-linux-gcc: Command not found

  解决：系统是64位，而程序是32位，所以我们需要安装32位兼容包，详细看1.8节课。

## 1.8.在ubuntu16.0403X64上安装海思交叉编译工具链

### 1.8.1、问题：工具链是32位的

(1)方法1：换32位ubuntu

(2)方法2：装32位兼容包

### 1.8.2、给ubuntu16.0403X64安装32位兼容包

(1)参考：http://blog.csdn.net/ma57457/article/details/68923623

或者：https://www.cnblogs.com/leaven/p/5084902.html

(2)用aptitude方式安装lib32z1。使用sudo apt-get install lib32z1，发现装不了。原因是ubuntu太新了，里面很多库都是新版本的，但是基于这个新版本的lib32z1还没有，所以不能装。解决方案就是用aptitude工具来装。看前导课程《嵌入式linux开发环境搭建》的第6节的6.3部分。

(3)测试执行arm-xxx-gcc -v，提示找不到stdc++错误：

./arm-hisiv300-linux-uclibcgnueabi-gcc -v

./arm-hisiv300-linux-uclibcgnueabi-gcc: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory

(4)再用aptitude方式安装lib32stdc++6-4.8-dbg

sudo aptitude install lib32stdc++6-4.8-dbg

再次测试arm-xxx-gcc -v，终于可以运行了。

 

**18.04安装32库**

sudo apt-get install lib32stdc++6*

sudo apt-get install lib32z1*

### 1.8.3、再次测试整体编译osdrv

(1)仍然提示找不到arm-hisiv300-linux-gcc

(2)修改Makefile中OSDRV_CROSS的路径，结果不行，分析原因是：命令行传参覆盖了

(3)export导出到环境变量，参考裸机课程1.4节，格式为：

export PATH=/home/aston/sambashare/Hi3518E_SDK_V1.0.3.0/osdrv/arm-hisiv300-linux/bin:$PATH

 

直接测试可以执行了，但是编译还是出错。分析原因：名字不对

(4)解决方案有2个：一种是修改make时传参的名字，另一种是给安装好的交叉编译工具链创建符号链接。实际尝试后发现第一种Makefile要改的太多，所以走第2种。

### 1.8.4、使用install脚本安装交叉编译工具链

(1)install到/opt目录下并建立符号链接

(2)导出到PATH并测试可以执行

(3)再次编译

**注意：**编译的增加 sudo make 会显示commond not found ,在root用户下也添加环境变量执行可以暂时解决这个问题。 如果有办法直接make 还有执行权限的话，那么有可能会解决commond not found 这个问题。

  直接在最外层将Hi3518E_SDK_V1.0.3.0 这个文件夹的权限更改为777,执行代码为:

  chmod -R 777 Hi3518E_SDK_V1.0.3.0

  这样就可以用 make来编译,而不用sudo make;

 

## 1.9.编译osdrv

### 1.9.1、编译策略和方法研究

(1)整体编译，遇到问题解决问题。

(2)部分编译，需要哪部分单独编译哪部分。

### 1.9.2、编译uboot 

### 1.9.3、编译kernel

## 1.10_11.编译rootfs1_2

### 1.10.1、缺zlib错误。

错误：compr_zlib.c:39:18: fatal error: zlib.h: No such file or directory

因为找不到zlib.h所以编译错误，zlib.h在tools/pc/zlib/tmp/include目录中有，只需要复制到tools/pc/jffs2_tool/tmp/include目录中即可。注意同时要将zconf.h也复制过去的。还有，要将tools/pc/jffs2_tool/tmp/lib目录下的libz.a libz.so libz.so.1 libz.so.1.2.7等4个文件复制到tools/pc/jffs2_tool/tmp/lib目录下，不然一会儿还得报错。

### 1.10.2、其他错误

还报错就不管了，此时已经有了mkfs.jffs2了，我们就是只要这个而已，手工将其复制到osdrv/pub/bin/pc目录下即可。

### 1.10.2_1、报错。

错误信息：serve_image.c:32:18: error: storage size of ‘hints’ isn’t known

解决方案，参考：http://blog.csdn.net/mtbiao/article/details/77052659

博客中的第二种方法:

**方法****2**.修改操作系统头文件/usr/include/netdb.h，将此宏__USE_XOPEN2K注释，如下图(注意#ifdef与#endif是一一对应的)

### 1.10.3、在32位ubuntu中编译时的一个错误

编译错误：/usr/bin/ld: i386:x86-64 architecture of input file `mkyaffs2image.o' is incompatible with i386 output

这个是因为sdk中本来就有在64位系统下编译的.o文件，而我们用的是32位的ubuntu，解决办法是进入tools/pc/mkyaffs2image/mkyaffs2image目录下，rm *.o（或者make clean）删除所有之前编译的痕迹即可。实际上我们板子上用的是spi flash，合适用jffs2文件系统，所以不会做yaffs2文件系统，这个东西要不要都无所谓的。

### 1.10.4、手工单独制作rootfs

制作参数为：

osdrv/pub/bin/pc/mkfs.jffs2 -d osdrv/pub/rootfs_uclibc -l -e 0x10000 -o osdrv/pub/rootfs_uclibc_64k.jffs2

### 1.10.5、编译最终完成的标准

在osdrv/pub/image_uclibc目录下得到的uboot和uImage即是我们要的uboot镜像和kernel的uImage镜像，而根文件系统镜像在osdrv/pub目录下。

注意：如果你的osdrv是完整编译结束的，那么得到的rootfs镜像中应该是不缺东西的。但是因为我做的时候是不完整编译的，rootfs是手工制作的，所以rootfs镜像中有一些缺陷。

 

### 1.10.6、还是出现一些错误

​    hipctools: prepare

  @echo "---------task [5] build tools which run on pc"

将makefile 这个目标里面的语句删掉只留下这一句。

 

## 1.12_13.uboot的烧写和flash分区1_2

### 1.12.1、裸机烧录uboot

(1)什么叫裸机烧录？设备是空白的，未经烧录的，就叫裸机。

(2)裸机烧录一个设备有2种方案：

1是用外部烧录器来烧录板载flash（外部烧录器烧录SPIFLASH时和HI3518E没有关系，有时候经常SPIFLASH先单独通过烧录器和支架来烧录好镜像，然后再把烧录过镜像的SPIFLASH焊接到板子上。现在很多烧录器也可以在板子上直接烧了）；

2是通过主芯片提供的isp下载的机制来间接烧录板载flash。现在这种方式常用；PC通过URT将数据传到3518E芯片然后再传给FLASH。3518E中BL0有isp相关代码；

(3)运行Hi_tool来烧录uboot

### 1.12.2、flash分区

(1)因为嵌入式系统为了简化，没有使用分区表来自动管理flash，所以都是事先定死的。所以在部署一个嵌入式系统前都要人为的定下一个分区

(2)原则1：每个分区要足够放镜像；原则2：尽量留一点扩展余地。原则3：在满足1、2情况下你随便搞。

(3)我定的分区：

  分区名     分区大小     起始地址    截至地址

  bootloader：  1M     0x00000000  0x00100000

  kernel：     3M     0x00100000  0x00400000

  rootfs:     12M     0x00400000  0x01000000

  

### 1.12.3、uboot的环境变量参数

### 1.12.4、各种常见flash的简单讲解

(1)买到的flash芯片，其实是内部的flash存储颗粒+外部封装的控制器来构成的。

(2)像EMMC、SD、MMC、SPIFLASH、NANDFLASH等差异都在于控制器。

(3)SPIFLASH·的优势就是接口简单，主芯片只需要支持SPI接口就可以外接。很多MCU或者CPU在需要外扩一个8M/16M/32M/64M这么大级别的外部存储器时，选择SPIFLASH是很好的。

(4)NANDFLASH其实控制器是最老的，像EMMC、SD等都比NANDFlash要更新一些，更好一些。

## 1.14.kernel和rootfs烧录与启动系统

### 1.14.1、烧录kernel

(1)SDRAM地址范围：80000000-83FFFFFF

(2)tftp得能通能下载，才能烧录。ip设置是：本地192.168.1.10，serverp是141

### 1.14.2、烧录rootfs

### 1.14.3、uboot的各环境变量介绍和设置

(1)网络地址：ipaddr 192.168.1.10， serverip 192.168.1.141

(2)bootcmd：

(3)bootargs：

附：烧录命令

\---------------------------------------------------

tftp更新并重新烧写uboot的命令序列：

mw.b 0x82000000 ff 0x100000　　　　＃0x100000　　是长度，不是终止地址

tftp 0x82000000 u-boot-hi3518ev200.bin

sf probe 0                #选择0通道的　SPI FLASH

sf erase 0x0 0x100000

sf write 0x82000000 0x0 0x100000

\--------------------------------------------------

tftp更新并重新烧写kernel的命令序列：

mw.b 0x82000000 ff 0x300000

tftp 0x82000000 uImage_hi3518ev200

sf probe 0

sf erase 0x100000 0x300000

sf write 0x82000000 0x100000 0x300000

\---------------------------------------------------

tftp更新并重新烧写rootfs的命令序列：

mw.b 0x82000000 ff 0xc00000

tftp 0x82000000 rootfs_hi3518ev200_64k.jffs2

sf probe 0

sf erase 0x400000 0xc00000

sf write 0x82000000 0x400000 0xc00000

 

附2：正确的bootcmd和bootargs对应的设置命令：

set bootcmd 'sf probe 0;sf read 0x82000000 0x100000 0x300000;bootm 0x82000000'

set bootargs mem=32M console=ttyAMA0,115200 root=/dev/mtdblock2 rootfstype=jffs2 mtdparts=hi_sfc:1024K(boot),3072K(kernel),12288K(rootfs)



 

  MTDPART表示分区表； 3518E芯片内嵌了64MB, 32MB给操作系统用,32MB给MPP用(用来处理视频编码也是需要消耗大量内存的)。

 

## 1.15.rootfs启动后做了什么

 

 

## 1.16_17.mpp的部署研究和实战1_2

### 1.16.1、mpp的文件结构详解

(1)ko

(2)lib

(3)sample

(4)其他几个

### 1.16.2、开发板启动自动挂载主机

(1)在profile中添加设置网卡IP地址

(2)在profile中添加自动挂载主机nfs服务器

mount -t nfs -o nolock 192.168.1.141:/home/aston/rootfs /mnt

自己设置的：mount -t nfs -o nolock 192.168.1.141:/root/rootfs_hs /mnt

 

### 1.16.3、部署ko文件 

./load3518e -i -sensor ar0130 -osmem 32 -total 64

### 1.16.4、部署lib文件

 

SDRAM范围：80000000-83FFFFFF

linux内存：0x80000000-0x81FFFFFF  MMZ内存：0x82000000-0x83FFFFFF

 

 

## 1.18.sample的编译和测试

### 1.18.1、sample的编译 

   先进入sample 底下修改Makefile.param的相关参数，主要有是哪款芯片和哪款sensor；

(1)sample结构简单浏览

(2)Makefile研究

(3)编译得到sample_venc

### 1.18.2、sample的部署和测试

(1)nfs方式运行sample

(2)得到录像文件xx.h264

(3)导出xx.h264到windows下用vlc播放器播放验证

### 1.18.3、镜像重新制作

(1)在编译目录的osdev/pub/下找到rootfs_uclibc.tgz，解压开

(2)按照上节和本节验证成功的部署步骤部署整个rootfs

(3)手工制作rootfs.jffs2镜像，再烧录测试即可

osdrv/pub/bin/pc/mkfs.jffs2 -d osdrv/pub/rootfs_uclibc -l -e 0x10000 -o osdrv/pub/rootfs_uclibc_64k.jffs2

### 1.18.4、最后的总结

(1)第1季结束，主要内容是海思SDK的熟悉和编译、部署实战

(2)目录要多翻，简单pdf文档要多看，概念要不断熟悉，熟能生巧，面试时也方便充经验

(3)重实战，不要只听课不动手

(4)回顾第1季入了门；展望第2季，详解sample程序，咬牙啃mpp编程手册，可登堂入室

 

 

 

 

 

 

 

 

 

 

# 第二章 从官方例程深度学习海思SDK及API

## 2.0章节概要

**2.1.官方mppsample的总体分析**

  本节分析官方SDK中mpp sample程序的总体情况，并且给出整个课程学习思路。

**2.2.图像像素格式深度理解1**

  本节从颜色学的角度讲解颜色的数学表达方法，提出亮度和色度分隔等关键概念。

**2.3.图像像素格式深度理解2**

  本节讲述rawRGB概念，并且详细讲了从光被反射进入镜头到最终成为图像数据的细节。

**2.4.RGB和YUV详解1**

  本节首先讲了RGB方式存储图像信息的原理，然后提出了YUV，并讲了YUV和RGB的异同点。

**2.5.RGB和YUV详解2**

  本节详细介绍了YUV的各种子格式，譬如什么是YUVSemiPlanar420，422这些。

**2.6.海思MPP功能模块和视频缓存池**

  本节带读MPP手册的系统部分，并且引入了一些数据结构并演示了如何查阅手册学习这些。

**2.7.视频缓存池**

  本节详细解释了视频缓存池这个概念，这是个关键性概念，对整个工作流程影响很大。

**2.8.程序流程分析和MPP初始化详解1**

  本节分析整个视频采集处理编码输出的完整流程。

**2.9.程序流程分析和MPP初始化详解2**

  本节重点讲解MPP系统本身初始化的流程。

**2.10.VI部分详解1**

  本节介绍了绘制函数调用图谱的分析学习方法，并且绘制了VI部分的调用图谱。

**2.11.VI部分详解2**

  本节重点讲了VI部分的Sensor操作和ISP模块，并分析了代码细节。

**2.12.VI部分详解3**

  本节讲解了VI部分宽动态和dev、chn等相关概念和程序细节。

**2.13.VPSS部分详解1**

  本节解读了MPP手册中VPSS部分，并且绘制了VPSS部分函数调用图谱。

**2.14.VPSS部分详解2**

  本节重点讲解VPSS的grp和chn等概念。

**2.15.VPSS部分详解3**

  本节细节讲解VPSS部分的代码执行流程。

**2.16.图像编码压缩基本原理**

  本节讲解图像编码压缩的一般原理，如空间冗余、时间冗余、视觉冗余等。

**2.17.MPP手册中图像编码部分解读**

  本节解读MPP手册中VENC模块相关部分讲述。

**2.18.sample中venc模块源码解读**

  本节讲解sample代码中venc部分的源码。

**2.19.编码后的流文件输出和课程总结**

  本节讲解最后的部分，即应用程序开一个读取现成select读取编码后的视频文件写成裸流视频文件。

## 2.1.官方mppsample的总体分析

### 2.1.1sample的整体架构

(1)sample其实是很多个例程，所以有很多个main

(2)每一个例程面向一个典型应用，common是通用性主体函数，我们只分析venc

(3)基本的架构是：venc中的main调用venc中的功能函数，再调用common中的功能函数，再调用mpp中的API，再调用HI3518E内部的硬件单元。

(4)sample的配置和编译，重点注意很多环境变量，目录结构不要乱动，参考第一季。

### 2.1.2、sample代码学习的关键

(1)得理解很多基础概念，譬如图像采集原理、模拟数字、通道、绑定等等

(2)得从宏观上理解整个视频采集、内部传递、处理、编码输出、网络传输等的过程。

(3)得反复看代码，熟才能生巧，才能帮助理解整个代码。

(4)得查阅mpp手册，熟悉海思这一套API的规矩和一般用法。

### 2.1.3、sample_venc的大体分析

(1)从main入手，main的传参分析

(2)几个基本概念：

  H.264 H.265 MJPEG 视频编码规范标准

  1080P、720P、VGA、D1 视频分辨率（清晰度）

  fps（frame per second） 帧率

  

  

## 2.2_3.图像像素格式深度理解1_2

### 2.2.1、颜色的学问

(1)颜色是主观还是客观存在？颜色的本质是光的波长，但是因为有人去看才有了颜色这个概念。（王阳明-汝未看此花时，此花与汝同归于寂；你既来看此花，则此花颜色一时明白起来，便知此花不在你心外）。

(2)颜色的三个关键：**亮度、色度**

亮度：亮度过高就刺眼；

色度：颜色是由亮度和色度共同表示的，色度是不包括亮度在内的颜色的性质，它反映的是颜色的色调和饱和度。

饱和度：

(3)人的眼睛并非理想完美的颜色识别器件，图像表达也有清晰度和质量高低的差异

(4)科学研究如何定义（或者表达、记录、计算）一种颜色？**色彩空间的概念**

### 2.2.2、rawRGB和图像采集过程

(1)图像采集的过程：光照在成像物体被反射->镜头汇聚->Sensor光电转换（模拟信号）->ADC为rawRGB

(2)sensor上每个像素只采集特定颜色的光的强度，因此sensor每个像素只能为R或G或B

(3)rawRGB和RGB都是用来描述图像的，图像采集时RGB是由rawRGB计算而来的

(4)因为图像颜色本身有一定连贯性，而且人眼是非理想的，因此图像采集和再显示给人这整个构成中有三个要素：分辨率（像素）、pitch（像素点之间的间距）、观看距离

(5)如果是视频，质量好坏还要加上帧率framerate

(6)图像的表达、压缩、修整等相关技术，就发生在rawRGB进来以后的各个环节

(7) rawRGB 与RGB的区别：

摄像头的数据输出格式一般分为CCIR601、CCIR656、RAW RGB等格式，此处说的RGB格式应该就是CCIR601或CCIR656格式。而RAW RGB格式与一般的RGB格式是有区别的。 

我们知道，Sensor的感光原理是通过一个一个的感光点对光进行采样和量化，但，在Sensor中，每一个感光点只能感光RGB中的一种颜色。所以，通常所说的30万像素或130万像素等，指的是有30万或130万个感光点。每一个感光点只能感光一种颜色。 

但是，要还原一个真正图像，需要每一个点都有RGB三种颜色，所以，对于CCIR601或656的格式，在Sensor模组的内部会有一个ISP模块，会将Sensor采集到的数据进行插值和特效处理，例如：**如果一个感光点感应的颜色是R，那么，ISP模块就会根据这个感光点周围的G、B感光点的数值来计算出此点的G、B值，那么，这一点的RGB值就被还原了，然后在编码成601或656的格式传送给Host。** 
 而RAW RGB格式的Sensor则是将没个感光点感应到的RGB数值直接传送给Host，由Host来进行插值和特效处理。

修正：

不对吧，我觉得还是根据周围GB点的感应值，来推算出此位置的GB值。可能sensor可以支持例如算法来修正R值：根据周围R值来综合得出此点的R值。得出此位置像素的RGB数值。 

Raw RGB 每个像素只有一种颜色（R、G、B中的一种）； 
 RGB 每个像素都有三种颜色，每一个的值在0~255之间； 
 在手机摄像头的测试过程中，由sensor输出的数据就是Raw data（Raw RGB），经过彩色插值就变成RGB

也不一定就是测试过程，想要获得真正的图像，都必须有的一个过程； 
 sensor输出的数据格式，主要分两种：YUV（比较流行），RGB，这就是sonsor的数据输出；这其中的GRB就是Raw RGB，是sensor的bayer阵列获取的数据（每种传感器获得对应的颜色亮度）； 
 但是输出的数据不等于就是图像的实际数据，模组测试时，就要写一个软件，完成数据采集（获得Raw data）－>彩色插值（目的是获得RGB格式，便于图像显示）－>图像显示； 
 这样就可以发现整个模组是否正常，有无坏点，脏点的等，检测出不良品；（软件的处理过程当中，为了获得更好的图像质量，还需要白平衡，gamma校正，彩色校正） 
 而在手机的应用中，手机根据相机模组的数据格式，提供一个ISP（主要用于RGB格的），配合软件，使照相功能得到应用；

 

## 2.4_5.RGB和YUV详解1_2

### 2.4.1、RGB方式表示颜色

(1)RGB有RGB565和RGB888，ARGB等多种子分类

(2)RGB的本质：将色度分解为R、G、B三部分，然后记录下亮度数据

(3)RGB的优势：方便数字化表达，广泛用于数字化彩色显示器，计算机编程等领域。

(4)RGB的劣势：和传统的灰度图兼容不好，表达颜色的效率不高，一个点要用RGB来表示。

### 2.4.2、YUV

(1)YUV是一种色彩空间，Y表示亮度，U和V表示色度。只有Y就是黑白图像，再加上UV就是彩色图像了。YUV的一个好处就是让彩色系统和传统黑白系统很好的兼容。

(2)YUV和RGB的相同点是：都是用来表达颜色的数学方法；不同点是：对颜色的描述思路和方法不同。RGB将一个颜色拆解为3个纯色的亮度组合，YUV将一个颜色分解为一个亮度和2个色度的组合。

(3)RGB和YUV之间可以用数学方法互相换算，是个典型的浮点运算过程。

(4)YUV和YCbCr几乎可以看做一个概念，详细的区分以后再去慢慢体会。

(5)YUV分为packed和planar两种。具体参考：

http://blog.csdn.net/sunnylgz/article/details/7580628

YUV 格式通常有两大类:打包(packed)格式和平面(planar)格式。前者将 YUV 分量存放在同一个数组中,通常是几个相邻的像素组成一个宏像素(macro-pixel);而后者使用三个数组分开存放 YUV 三个分量,就像是一个三维平面一样。

 

(6)有多种YUV相关的概念需要弄清楚

YUV

YUYV :

YUV422

YUV420(YUV411)

YUV422 planar(YUV422P)

YUV420 Planar(YUV420P)

YUV422 semi planar(YUV422SP)

YUV420 semi Planar(YUV420SP)

参考：http://blog.csdn.net/bingqingsuimeng/article/details/50716390

和https://www.2cto.com/kf/201303/198023.html

### 2.4.3、YUV图解 （YUV444, YUV422, YUV420, YV12, NV12, NV21）

YUV格式有两大类：planar和packed。

对于planar的YUV格式，先连续存储所有像素点的Y，紧接着存储所有像素点的U，随后是所有像素点的V。对于packed的YUV格式，每个像素点的Y,U,V是连续交*存储的。

 

YUV，分为三个分量，“Y”表示明亮度（Luminance或Luma），也就是灰度值；而“U”和“V” 表示的则是色度（Chrominance或Chroma），作用是描述影像色彩及饱和度，用于指定像素的颜色。 与我们熟知的RGB类似，YUV也是一种颜色编码方法，主要用于电视系统以及模拟视频领域，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的图像，只不过是黑白的，这样的设计很好地解决了彩色电视机与黑白电视的兼容问题。并且，YUV不像RGB那样要求三个独立的视频信号同时传输，所以用YUV方式传送占用极少的频宽。

YUV码流的存储格式其实与其采样的方式密切相关，主流的采样方式有三种，YUV4:4:4，YUV4:2:2，YUV4:2:0，关于其详细原理，可以通过网上其它文章了解，这里我想强调的是如何根据其采样格式来从码流中还原每个像素点的YUV值，因为只有正确地还原了每个像素点的YUV值，才能通过YUV与RGB的转换公式提取出每个像素点的RGB值，然后显示出来。

 

​                                   用三个图来直观地表示采集的方式吧，以黑点表示采样该像素点的Y分量，以空心圆圈表示采用该像素点的UV分量。

先记住下面这段话，以后提取每个像素的YUV分量会用到。

 

**YUV 4:4:4采样，每一个Y对应一组UV分量。**

**YUV 4:2:2采样，每两个Y共用一组UV分量。**

**YUV 4:2:0采样，每四个Y共用一组UV分量。** 

\2. **存储方式**

 

  下面我用图的形式给出常见的YUV码流的存储方式，并在存储方式后面附有取样每个像素点的YUV数据的方法，其中，**Cb、Cr的含义等同于U、V**。

 

   （1） **YUVY 格式 （属于YUV422）**

 

YUYV为YUV422采样的存储格式中的一种，相邻的两个Y共用其相邻的两个Cb、Cr，分析，对于像素点Y'00、Y'01 而言，其Cb、Cr的值均为 Cb00、Cr00，其他的像素点的YUV取值依次类推。

 

 

 

​    （2） **UYVY 格式 （属于YUV422）**

UYVY格式也是YUV422采样的存储格式中的一种，只不过与YUYV不同的是UV的排列顺序不一样而已，还原其每个像素点的YUV值的方法与上面一样。

 

   （3） **YUV422P（属于YUV422）**

YUV422P也属于YUV422的一种，它是一种Plane模式，即平面模式，并不是将YUV数据交错存储，而是先存放所有的Y分量，然后存储所有的U（Cb）分量，最后存储所有的V（Cr）分量，如上图所示。其每一个像素点的YUV值提取方法也是遵循YUV422格式的最基本提取方法，即两个Y共用一个UV。比如，对于像素点Y'00、Y'01 而言，其Cb、Cr的值均为 Cb00、Cr00。

 

 

 

 

 

 

 

（4）**YV12，YU12格式（属于YUV420）**

   **YU12和YV12属于YUV420格式**，也是一种Plane模式，将Y、U、V分量分别打包，依次存储。其每一个像素点的YUV数据提取遵循YUV420格式的提取方式，即4个Y分量共用一组UV。注意，上图中，Y'00、Y'01、Y'10、Y'11共用Cr00、Cb00，其他依次类推。

   （5）**NV12、NV21（属于YUV420）**

**NV12和NV21属于YUV420格式**，是一种two-plane模式，即Y和UV分为两个Plane，但是UV（CbCr）为交错存储，而不是分为三个plane。其提取方式与上一种类似，即Y'00、Y'01、Y'10、Y'11共用Cr00、Cb00

 

YUV420 planar数据， 以720×488大小图象YUV420 planar为例，

其存储格式是： 共大小为(720×480×3>>1)字节，

分为三个部分:Y,U和V

Y分量：  (720×480)个字节 

U(Cb)分量：(720×480>>2)个字节

V(Cr)分量：(720×480>>2)个字节

三个部分内部均是行优先存储，三个部分之间是Y,U,V 顺序存储。

 

即YUV数据的0－－720×480字节是Y分量值，    

720×480－－720×480×5/4字节是U分量   

720×480×5/4 －－720×480×3/2字节是V分量。

4 ：2： 2 和4：2：0 转换：

**最简单的方式：**

YUV4:2:2 ---> YUV4:2:0 Y不变，将U和V信号值在行(垂直方向)在进行一次隔行抽样。 YUV4:2:0 ---> YUV4:2:2 Y不变，将U和V信号值的每一行分别拷贝一份形成连续两行数据。

 

在YUV420中，一个像素点对应一个Y，一个4X4的小方块对应一个U和V。对于所有YUV420图像，它们的Y值排列是完全相同的，因为只有Y的图像就是灰度图像。YUV420sp与YUV420p的数据格式它们的UV排列在原理上是完全不同的。420p它是先把U存放完后，再存放V，也就是说UV它们是连续的。而420sp它是UV、UV这样交替存放的。(见下图) 有了上面的理论，我就可以准确的计算出一个YUV420在内存中存放的大小。 width * hight =Y（总和） U = Y / 4  V = Y / 4

 

 

 

所以YUV420 数据在内存中的长度是 width * hight * 3 / 2，

假设一个分辨率为8X4的YUV图像，它们的格式如下图：

​       

 

​           

 

 

 

 

图 YUV420sp格式

 

 

 

[     ](http://static.oschina.net/uploads/img/201412/08141455_7BIM.png)

​            图 YUV420p数据格

 

**旋转90度的算法:**

public static void rotateYUV240SP(byte[] src,byte[] des,int width,int height)

 {

  

 int wh = width * height;

 *//**旋转**Y*

 int k = 0;

 for(int i=0;i<width;i++) {

  for(int j=0;j<height;j++) 

  {

​        des[k] = src[width*j + i];  

​     k++;

  }

 }

 

 for(int i=0;i<width;i+=2) {

  for(int j=0;j<height/2;j++) 

  { 

​        des[k] = src[wh+ width*j + i]; 

​        des[k+1]=src[wh + width*j + i+1];

​     k+=2;

  }

 }

 

 

 }

 

 

 

**YV12和I420的区别**   

 一般来说，直接采集到的视频数据是RGB24的格式，RGB24一帧的大小size＝width×heigth×3 Bit，RGB32的size＝width×heigth×4，如果是I420（即YUV标准格式4：2：0）的数据量是 size＝width×heigth×1.5 Bit。    在采集到RGB24数据后，需要对这个格式的数据进行第一次压缩。即将图像的颜色空间由RGB2YUV。因为，X264在进行编码的时候需要标准的YUV（4：2：0）。但是这里需要注意的是，虽然YV12也是（4：2：0），但是YV12和I420的却是不同的，在存储空间上面有些区别。如下：

YV12 ： 亮度（行×列） ＋ U（行×列/4) + V（行×列/4）

I420 ： 亮度（行×列） ＋ V（行×列/4) + U（行×列/4）

可以看出，YV12和I420基本上是一样的，就是UV的顺序不同。

继续我们的话题，经过第一次数据压缩后RGB24－>YUV（I420）。这样，数据量将减少一半，为什么呢？呵呵，这个就太基础了，我就不多写了。同样，如果是RGB24－>YUV（YV12），也是减少一半。但是，虽然都是一半，如果是YV12的话效果就有很大损失。然后，经过X264编码后，数据量将大大减少。将编码后的数据打包，通过RTP实时传送。到达目的地后，将数据取出，进行解码。完成解码后，数据仍然是YUV格式的，所以，还需要一次转换，这样windows的驱动才可以处理，就是YUV2RGB24:

YUY2 是 4:2:2 [Y0 U0 Y1 V0]

 

**yuv420p 和 YUV420的区别 在存储格式上有区别:**

yuv420p：yyyyyyyy uuuuuuuu vvvvv yuv420： yuv yuv yuv

   YUV420P，Y，U，V三个分量都是平面格式，分为I420和YV12。I420格式和YV12格式的不同处在U平面和V平面的位置不同。**在I420格式中，U平面紧跟在Y平面之后，然后才是V平面（即：YUV）；但YV12则是相反（即：YVU）**。

YUV420SP, Y分量平面格式，UV打包格式, 即NV12。 NV12与NV21类似，U 和 V 交错排列,不同在于UV顺序。

I420: YYYYYYYY UU VV  =>YUV420P

YV12: YYYYYYYY VV UU  =>YUV420P

NV12: YYYYYYYY UVUV   =>YUV420SP

NV21: YYYYYYYY VUVU   =>YUV420SP

[1] 参考: https://blog.csdn.net/xjhhjx/article/details/80291465

 

## 2.6.海思MPP功能模块和视频缓存池

### 2.6.1、MPP功能模块

(1)找到MPP手册

(2)详见系统概述1.3部分

### 2.6.2、sample中SAMPLE_VENC_1080P_CLASSIC函数开始看

(1)PAYLOAD_TYPE_E

(2)PIC_SIZE_E

(3)VB_CONF_S

 

注: PAYLOAD_TYPE_E enPayLoad[3]= {PT_H264, PT_H264,PT_H264};

 TYPE_E 表示枚举,TYPE_S表示结构体,数组enPlayLoad中的en表示枚举;

 

## 2.7.视频缓存池 (video buffer)

### 2.7.1、什么是视频缓冲池

视频缓存池主要向媒体业务提供大块物理内存管理功能，负责内存的分配和回收，充
 分发挥内存缓存池的作用，让物理内存资源在各个媒体处理模块中合理使用。
 一组大小相同、物理地址连续的缓存块组成一个视频缓存池。
 视频输入通道需要使用公共视频缓存池。所有的视频输入通道都可以从公共视频缓存
 池中获取视频缓存块用于保存采集的图像（如图 2-1 中所示从公共视频缓存池 A 中获
 取视频缓存块 Bm）。由于视频输入通道不提供创建和销毁公共视频缓存池功能，因
 此，在系统初始化之前，必须为视频输入通道配置公共视频缓存池。根据业务的不
 同，公共缓存池的数量、缓存块的大小和数量不同。 图 2-1 中所示缓存块的生存期是
 指经过 VPSS 通道传给后续模块的情形（ 图 2-1 实线路径）。如果该缓存块完全没有经
 过 VPSS 通道传给其他模块，则将在 VPSS 模块处理后被放回公共缓存池（ 图 2-1 虚线
    路径）。

图2-1 典型的公共视频缓存池数据流图

 

(1)视频的本质是多帧图片，图片的本质是RGB或rawRGB数据，要占用一段连续内存

(2)视频的裁剪、缩放、修正处理等各种操作，本质上就是对内存中的数据进行运算

(3)视频缓存池(VB, video buffer)就是一段很大，又被合理划分和管理的内存，用来做视频数据的暂存和运算场地

(4)公共视频缓存池的公共2字，可以理解为全局变量，也就是各个模块都能访问的一段内存

 

(5)看似视频缓存块在各个模块之间流转，实际上并没有内存复制，而是指针在传递

(6)视频缓存池的内存由MPP来维护，我们在系统启动时就把整个SDRAM分成了2部分：系统部分（由linux kernel来维护管理）和mpp部分（由mpp系统来维护管理）

(7)缓存池需要几个，每个中包含几个缓存块，每个缓存块多大，都是可以由用户程序设置好参数，然后调用MPP的相应API来向MPP申请分配的。

### 2.7.2、相关的数据结构和API

(1)VB_CONF_S        定义视频缓存池属性结构体

(2)HI_MPI_VB_SetConf     设置 MPP 视频缓存池属性

(3)HI_MPI_VB_Init      初始化 MPP 视频缓存池

 

 

 

## 2.8_9.程序流程分析和MPP初始化详解1_2

### 2.8.1、整个main流程分析 

 

step 1: init sys variable (MPP system) 

step 2: mpp system init.

step 3: start vi dev & chn to capture

step 4: start vpss and vi bind vpss

step 5: start stream venc

step 6: stream venc process -- get stream, then save it to file.

step 7: exit process

### 2.8.2、MPP系统初始化详解

(1)什么是MPP系统

 

(2)MPP系统为什么要初始化  

(3)MPP系统初始化为什么要在最前面

(4)MPP系统怎样进行初始化

(5)MPP系统初始化尤其注意API调用的顺序

 

 

## 2.10.VI部分详解1

### 2.10.1、上节遗留问题

(1)CEILING_2_POWER宏的作用

(2)PIXEL_FORMAT_YUV_SEMIPLANAR_420是怎么定出来的

### 2.10.2、学习方法：绘制调用关系图谱

(1)简单浏览VI部分的调用层次，发现很复杂

(2)有些函数是sample写的，有些是调用MPP的，数据结构也是2种都有

(3)学习重点1：全局把控熟悉整个过程全景视图

(4)学习重点2：掌握细节数据结构元素含义，和遇到的概念

(5)学习重点3：知道某些关键操作在哪里定义，哪里设置，将来需要改的时候能找到地方

### 2.10.3、绘制VI部分调用关系

[sample_venc.c.pdf](file:///C:/Users/administer/AppData/Roaming/Microsoft/Word/sample_venc.c.pdf)

## 2.11_12.VI部分详解2_3

(1)常用Sensor的接口有三种：MIPI、LVDS、DC

(2)WDR宽动态:

宽动态技术是在非常强烈的对比下让摄像机看到影像的特色而运用的一种技术。

当在强光源（日光、灯具或反光等）照射下的高亮度区域及阴影、逆光等相对亮度较低的区域在图像中同时存在时，摄像机输出的图像会出现明亮区域因曝光过度成为白色，而黑暗区域因曝光不足成为黑色，严重影响图像质量。摄像机在同一场景中对最亮区域及较暗区域的表现是存在局限的，这种局限就是通常所讲的“动态范围”。

 

(3)isp就是image signal process，图像信号处理。

(4)HI3518E内部的ISP单元是隶属于VI模块的。VI模块就包含3大部分：第一部分是和Sensor对接的部分，第二部分就是ISP，第三部分就是VI dev和channel

(5) AE [**automatic**](javascript:;) [**exposure**](javascript:;):自动曝光

(6) AWB Automatic white balance: 自动白平衡

(7) AF automatic focusing :自动对焦

 

## 2.13.VPSS部分详解1

### 2.13.1、VPSS的手册部分解读

**z.VPSS 概念**

VPSS（ Video Process Sub-System）支持对一幅输入图像进行统一预处理，如去噪、去隔行等，然后再对各通道分别进行缩放、锐化等处理，最后输出多种不同分辨率的图像。

 VPSS 单元支持的具体图像处理功能包括 FRC（ Frame Rate Control）、 Crop、 NR（ Noise Reduce）、 LDC（ Lens Distortion Correction）、 Rotate、 Cover/Overlay、 Scale、Mirror/Flip、 FishEye 等。

**z.****GROUP**

VPSS 对用户提供组（ GROUP）的概念。最大可用数为 VPSS_MAX_GRP_NUM个，各芯片的最大

组数目有所不同，各 GROUP 分时复用 VPSS 硬件。每个 VPSSGROUP 包含多个通道，通道数目视方案实现有所不同，具体描述请参见CHANNEL。
 z **CHANNEL**

VPSS 组的通道。通道分为 2 种：物理通道和扩展通道。 VPSS 硬件提供多个物理通道，每个通道具有缩放、裁剪等功能。扩展通道具备缩放功能，它通过绑定物理通道，将物理通道输出作为自己的输入，把图像缩放成用户设置的目标分辨率输出。

### 2.13.2、VPSS的函数调用关系图谱

[sample_venc.c.pdf](file:///C:/Users/administer/Desktop/笔记/word/海思编解码项目/sample_venc.c.pdf)

## 2.14_15.VPSS部分详解2_3

### 2.13.3、VPSS的Grp和Chn

(1)VPSS的Grp

(2)VPSS的Chn

(3)VI的Chn（和Dev）

### 2.13.4、VPSS部分代码详解

 

 

## 2.16.图像编码压缩基本原理

### 2.16.1 图像可压缩的原因

一张原始图像(1920x1080)，如果每个像素32bit表示（RGBA），那么，图像需要的内存大小1920x1080x4 = 8294400 Byte，大约8M。这我们是万万不能接受的。如果这样，1G硬盘才存100多张图片，伤不起啊！视频也一样，如果视频是1920x1080，30fps， 1小时。那不压缩大概需要的内存：

8Mx30x60*60 = 864000M，都800多G了！疯了吧！

所以说，我们需要图像压缩:

那图像为何可以压缩呢？因为它有很多冗余信息。

常见图像、视频、音频数据中存在的冗余类型如下：

\1. 空间冗余

\2. 时间冗余

\3. 视觉冗余

下面详细介绍。

 

 

 

**1.1空间冗余**

一幅图像表面上各采样点的颜色之间往往存在着空间连贯性，比如下图，两只老鼠的颜色，背后的墙，灰色的地板，颜色都一样。这些颜色相同的块就可以压缩。

比如说，第一行像素基本都一样，假设亮度值Y是这么存的:

[105 105 105…….105]，如果共100个像素，那需要1Byte*100。

最简单的压缩：[105, 100]，表示接下来100个像素的亮度都是105，那么只要2个字节，就能表示整行数据了！岂不是压缩了！

  

 

 

 

 

 

 

 

 

 

 

**1.2 时间冗余**

这种冗余主要针对视频。

运动图像（视频）一般为位于一时间轴区间的一组连续画面，其中的相邻帧往往包含相同的背景和移动物体，只不过移动物体所在的空间位置略有不同，所以后一帧的数据与前一帧的数据有许多共同的地方，这种共同性是由于相邻帧记录了相邻时刻的同一场景画面，所以称为时间冗余。

如下图所示，其实1秒30帧，每一帧之间都是33ms，这么短，前后帧的变话很少，也许只有嘴巴动了，背景没动

 

 

 

 

  

**1.3 视觉冗余**

人类的视觉系统由于受生理特性的限制，对于图像场的注意是非均匀的，人对细微的颜色差异感觉不明显。

例如，人类视觉的一般分辨能力为26灰度等级，而一般的图像的量化采用的是28灰度等级，即存在视觉冗余。

人类的听觉对某些信号反映不太敏感，使得压缩后再还原有允许范围的变化，人也感觉不出来。

### 2.16.2 数据压缩方法的分类

**1.无失真压缩**

无失真压缩要求解压以后的数据和原始数据完全一致。解压后得到的数据是原数据的复制，是一种可逆压缩。

无失真压缩法去掉或减少数据中的冗余，恢复时再重新插到数据中，因此是可逆过程

根据目前的技术水平，**无损压缩算法一般可以把普通文件的数据压缩到原来的1/2－1/4**。一些常用的无损压缩算法有赫夫曼(Huffman)算法和LZW(Lenpel-Ziv & Welch)压缩算法

 

 

**2.有失真压缩**

解压以后的数据和原始数据不完全一致，是不可逆压缩方式。有失真压缩还原后，不影响信息的表达。

例如，图像、视频、音频数据的压缩就可以采用有损压缩方法，因为其中包含的数据往往多于我们的视觉系统和听觉系统所能接收的信息，丢掉一些数据而不至于对声音或者图像所表达的意思产生误解，但可大大提高压缩比。图像、视频、音频数据的压缩比可高达100:1，但人的主观感受仍不会对原始信息产生误解。

 

### 2.16.3 按照压缩方法的原理分类

1 预测编码

基本思想是利用已被编码的点的数据值，预测邻近的一个像素点的数据值

2 变换编码

基本思想是将图像的光强矩阵变换到系数空间上，然后对系数进行编码压缩

3 统计编码

根据信息出现概率的分布特性而进行的压缩编码。比如霍夫曼编码。

 

### 2.16.4图像压缩的要素

**压缩比**

压缩前后文件大小之比，越高越好，但受速度、消耗资源等的影响。

**图像质量**

还原后与原图像相比，评估的方法有客观评估和主观评估。

**压缩与解压缩速度**

与压缩方法和压缩编码的算法有关，一般压缩比解压缩计算量大，因而压缩比解压缩慢。

 

 

### 2.16.5. 图像压缩编码举例

**1 行程编码（RLE）**

这是最好理解的一种编码了。

现实中有许多这样的图像，在一幅图像中具有许多颜色相同的图块。在这些图块中，许多行上都具有相同的颜色，或者在一行上有许多连续的像素都具有相同的颜色值。在这种情况下就不需要存储每一个像素的颜色值，而仅仅存储一个像素的颜色值，以及具有相同颜色的像素数目就可以，或者存储像素的颜色值，以及具有相同颜色值的行数。

这种压缩编码称为行程编码(run length encoding，RLE)，具有相同颜色并且是连续的像素数目称为行程长度。

例如,字符串AAABCDDDDDDDDBBBBB

利用RLE原理可以压缩为3ABC8D5B

RLE编码简单直观，编码/解码速度快，

因此许多图形和视频文件，如.BMP .TIFF及AVI等格式文件的压缩均采用此方法.

由于一幅图像中有许多颜色相同的图块，用一整数对存储一个像素的颜色值及相同颜色像素的数目（长度）。例如：

（G ，L）//G为颜色值，L为长度值

编码时采用从左到右，从上到下的排列，每当遇到一串相同数据时就用该数据及重复次数代替原来的数据串。

 

举例，如下的18*7的像素（假设只有灰度值，1字节）

000000003333333333

222222222226666666

111111111111111111

111111555555555555

888888888888888888

555555555555553333

222222222222222222

仅仅需要11对数据表示。

(0,8) (3,10) (2,11) (6,7)

(1,18) (1,6) (5,12) (8,18)

(5,14) (3,4) (2,18)

 

游程长度编码特点：

直观，经济；

是一种无损压缩；

压缩比取决于图像本身特点，相同颜色图像块越大，图像块数目越少，压缩比越高。

适用于计算机生成的图像，例如。BMP、TIF等，不适于颜色丰富的自然图像。

 

这并不是说RLE编码方法不适用于自然图像的压缩，相反，在自然图像的压缩中少不了RLE，只不过是不能单纯使用RLE一种编码方法，需要和其他的压缩编码技术联合应用。

 

**2 哈夫曼编码（Huffman)**

由于图像中表示颜色的数据出现的概率不同，对于出现频率高的赋（编）予较短字长的码，对出现频率小的编于较长字长的码，从而减少总的代码量，但不减少总的信息量。

 

编码步骤：

(1)初始化，根据符号概率的大小按由大到小顺序对符号进行排序

(2)把概率最小的两个符号组成一个节点，如图4-02中的D和E组成节点P1。

(3)重复步骤2，得到节点P2、P3和P4，形成一棵“树”，其中的P4称为根节点。

(4)从根节点P4开始到相应于每个符号的“树叶”，从上到下标上“0”(上枝)或者“1”(下枝)，至于哪个为“1”哪个为“0”则无关紧要，最后的结果仅仅是分配的代码不同，而代码的平均长度是相同的。

(5)从根节点P4开始顺着树枝到每个叶子分别写出每个符号的代码。

 

**3 DCT编码**

3.1 基本概念

将在空域上描述的图象，经过某种变换（通常采用，余弦变换、傅立叶变换、沃尔什变换等），在某种变换域里进行描述。

在变换域里，首先降低了图象的相关性；其次通过某种图象处理（如频域的二维滤波）以及熵编码，则可进一步压缩图象的编码比特率。

这种变换常用于JPEG图像压缩。

G : 输入源图像
 G’ :解码后的图像
 U： 二维正交变换
 U’ : 二维正交逆变换

 

   3.2 变换压缩原理框图

 

G : 输入源图像
 G’ :解码后的图像
 U： 二维正交变换
 U’ : 二维正交逆变换

————————————————

版权声明：本文为CSDN博主「长江很多号」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：https://blog.csdn.net/newchenxf/article/details/51693753

 

## 2.17.MPP手册中图像编码部分解读

2.17.1 概述

VENC 模块，即视频编码模块。本模块支持多路实时编码，且每路编码独立，编码协
 议和编码 profile 可以不同。本模块支持视频编码同时，调度 Region 模块对编码图像内
 容进行叠加和遮挡。

**VENC** **模块的输入源包括三类：**
 z 用户态读取图像文件向编码模块发送数据；
 z 视频输入（ VIU）模块采集的图像经视频处理子系统（ VPSS）发送到编码模块；
 z 视频输入（ VIU）模块采集的图像直接发送到编码模块；

 

2.17.2 码率控制

  1什么是码率？

视频码率就是数据传输时单位时间传送的数据位数，一般我们用的单位是kbps即千位每秒。通俗一点的理解就是取样率，单位时间内取样率越大，精度就越高，处理出来的文件就越接近原始文件。

但是文件体积与取样率是成正比的，所以几乎所有的编码格式重视的都是如何用最低的码率达到最少的失真，围绕这个核心衍生出来的cbr（固定码率）与vbr（可变码率），都是在这方面做的文章，不过事情总不是绝对的，举例来看，对于一个音频，其码率越高，被压缩的比例越小，音质损失越小，与音源的音质越接近。

2 可变码率和固定码率编码

与固定码率视频编码不同的是，可变码率视频编码能够根据输入视频信号的特性以恒定图像质量和可变的码率进行传输。其中，视频压缩算法可以是预测编码、变换编码、子带编码和矢量量化等。 [1] 

可变码率编码

从确保视频传输质量和充分利用信息的角度来说，**可变码率视频编码才是最合理的**。其理由是：视频信源本身的高峰信息量是变化的，若要使其输出码流的码率固定不变，则需要按信源的高峰信息量去设计传输系统，但大部分时间并不出现高峰信息量，为了确保码率固定，通常要插入一些填充码，这就浪费了视频资源。使用可变码率编码，就能按信源本身的信息量去分配，从而有效地利用信息资源。 [1] 

固定码率编码

通常固定码率视频编码算法是通过利用缓冲器状态改变量化器步长来实现的。当图像细节丰富时，为了确保缓冲器不溢出，就得加大量化步长，减少编码比特数，从而保证输出码流的码率恒定。这是以牺牲视频质量为代价的。因为加大量化步长会损伤图像的高频细节和低频背景，甚至产生“块效应”和图像细节模糊现象。所以当你用VCD机播放节目时，留心观察，就会发觉有时图像边缘出现“影环”现象。


   2码率控制器实现对编码码率进行控制。

从信息学的角度分析，图像的压缩比越低，压缩图像的质量越高；图像压缩比例越高，压缩图像的质量越低。对于场景变化的真实场景，图像质量稳定，编码码率会波动；编码码率稳定，图像质量会波动。以 H.264 编码为例，通常图像 Qp 越低，图像的质量越好，码率越高；图像 Qp 越高，图像质量越差，码率越低。码率控制是针对连续的编码码流而言，所以， JPEG 协议编码通道不包括码率控制功能。码率控制器分别提供了对 H.264\H.265\MJPEG 协议编码通道 CBR、 VBR、 FIXQP 等三种码率控制模式，对图像质量和码率进行调节。
 Hi3518EV200 不支持 H.265 编码，所以，也不支持 H.265 类型的码率控制。

**1.CBR**
 CBR（ Constant Bit Rate）固定比特率。即在码率统计时间内保证编码码率平稳。码率稳定主要由两个量来评估，这两个量都可以由用户在创建编码通道时指定。
 z **码率统计时间** **u32StatTime**
   单位为秒(s)，码率统计时间越长，每帧图像的码率波动对于码率调节的影响越弱，码率的调节会更缓慢，图像质量的波动会更轻微；码率统计时间越短，每帧图像的码率波动对于码率调节的影响越强，图像码率的调节会更灵敏，图像质量的波动会更剧烈。
 z **行级码率控制调节幅度** **u32RowQpDelta**
 行级码率控制调节幅度是一帧内行级调节的最大范围，其中行级以宏块行为单位。调节幅度越大，允

许行级调整的 QP 范围越大，码率越平稳。对于图像复杂度分布不均匀的场景，行级码率控制调节幅度设置过大会带来图像质量不均匀。

**2.VBR**
 VBR（ Variable Bit Rate）可变比特率，即允许在码率统计时间内编码码率波动，从而保证编码图像质量平稳。以 H.264 编码为例， VENC 模块提供用户可设置 MaxQp，MinQp， MaxBitrate 和 ChangePos。 MaxQp， MinQp 用于控制图像的质量范围，MaxBitrate 用于钳位码率统计时间内的最大编码码率， ChangePos 用于控制开始调整Qp 的码率基准线。当编码码率大于 MaxBitrate*ChangePos 时，图像 qp 会逐步向MaxQp 调整，如果图像 QP 达到 MaxQp， QP 会被钳位到最大值， MaxBitrate 的钳位效果失效，编码码率有可能会超出 MaxBitrate。当编码码率小于 MaxBitrate*ChangePos时，图像 QP 会逐步向 MinQp 调整，如果图像 QP 达到 MinQp，此时编码的码率已经达到最大值，而且图像质量最好。
 **3.FIXQP**
  Fix Qp 固定 Qp 值。在码率统计时间内，编码图像所有宏块 Qp 值相同，采用用户设定
 的图像 Qp 值， I 帧和 P 帧的 QP 值可以分别设置。

Qp概念：参考：http://blog.csdn.net/u013354805/article/details/51988171

码率视频讲解：https://www.vmovier.com/43744

## 2.18.sample中venc模块源码解读

## 2.19.编码后的流文件输出和课程总结

demo.spd

m=video 8080 RTP/AVP 96

a=rtpmap:96 H264

a=framerate:25

c=IN IP4 192.168.1.20

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

# 第三章 ORTP库局域网图传和VLC实时预览

## 3.0.章节概要

**3.1.ORTP的引入**

  本节讲述基于下载和基于实时2种视频传输策略，并引入ortp库和RTP协议。

**3.2.ORTP库的移植**

  本节下载并配置交叉编译移植ortp库到当前环境中。

**3.3.RTP传输视频实战**

  本节添加事先编写好的调用ortp库实现视频传输的代码，并且部署运行测试。

**3.4.ORTP库的源码分析1**

  本节分析ortp库的源码，主要是以src/tests/rtpsend.c这个案例来分析的。

**3.5.ORTP库的源码分析2**

  本节深度分析ortp库的源码，尤其是rtp发送函数内部的层层递进调度关系。

**3.6.RTP发送实验源码分析1** 

  本节分析自己编写的rtp发送函数部分，主要解释了程序流程和一些重要参数。

**3.7.RTP发送实验源码分析2**

  本节继续分析自己编写的rtp发送函数部分，尤其是分包进行rtp发送的部分。

**3.4.8.VLC的sdp文件解析和课程总结**

  本节分析VLC播放器的sdp文件格式，并对第三季课程进行总结。

 

## 3.1.ORTP的引入

### 3.1.1、上季回顾

(1)搭建了SDK开发环境，编译系统，部署并测试环境

(2)详解了sample源码，编译运行，测试录像，验证硬件

 

 

 

### 3.1.2、视频网络传输的2种方式

(1)基于下载：http or ftp(应用层)

   如果网速慢了,他会卡在那,等到缓冲好了再播放。适合用于视频网站

(2)基于实时：RTP/RTSP/RTCP

  RTP  ：实时传输协议（Real Time Protocol）；

  RTSP  ：实时流传输协议（real time streaming protocol）；

  RTCP  ：实时控制协议(Real-time Transport Control Protocol）；

  如果网速不够牺牲画面质量，它一直是实时的画面；如果卡住了，过段时间画面接上了，也是实时的画面，不是接上卡住的那段画面。

### 3.1.3、ORTP的介绍

(1)openRTP，用C实现的一个RTP库（其实还有C++实现的，JAVA等实现的）

(2)实质是一个视频服务器，工作时客户端和服务器实时传递视频数据

(3)一般认为RTP工作在传输层，但是其实RTP比TCP/UDP高一个层次

(4)RTP（及RTCP）的实现有国际标准RFC3550规定，只要符合协议谁都可以自己写一个

(5)本季课程重点在于使用ORTP来实现局域网视频实时传输

 

 

## 3.2.ORTP库的移植

### 3.2.1、准备源码

(1)下载ortp源码：https://github.com/dmonakhov/ortp

(2)存放到临时工作目录并解压

### 3.2.2、源码修改

(1)增加H.264的payload支持。

在src/avprofile.c中357行添加：

rtp_profile_set_payload(profile,96,&payload_type_h264);

### 3.2.3、配置和编译、安装

(1)进入ortp目录执行./autogen.sh

(2)错误1：./autogen.sh: line 44: libtoolize: command not found

  解决：sudo apt-get install libtool*

(2)错误2：libtoolize:  error: Please install GNU M4, or 'export M4=/path/to/gnu/m4'.

  解决：sudo apt-get install m4

(3)错误3：Automake - aclocal: command not found 

  解决：sudo apt-get install automake

(4)继续执行./configure --prefix=/tmp/ortp --host=arm-hisiv300-linux

(5)make && make install

 

### 3.2.4、到/tmp/ortp目录下查看移植好的库和头文件

### 3.2.5、Ubuntu18.04遇到的问题以及解决方法

 

 

配置的时候将PKG_CHECK_MODULES(LIBZRTPCPP, LIBZRTPCPP >= 2.0.0)注释掉，然后再make && make install

 

## 3.3.RTP传输视频实战

### 3.3.1、在官方SDK的sample中添加rtp传输代码

(1)venc/sample_venc.c中，添加：s32ChnNum = 1;

(2)common/sample_common_venc.c中，改了很多

### 3.3.2、重新编译sample

(1)复制ortp头文件

(2)修改venc中Makefile，添加libortp的链接支持

(3)make

### 3.3.3、开发板中部署并运行测试

(1)部署libortp.so到开发板中/usr/lib目录下

(2)检查开发板中原有配置是否正确，譬如sensor是否对应实际

(3)在nfs中运行新的sample程序

(4)vlc中打开配置好的sdp文件，看到实时图像就证明整个实验完成了

 

 

## 3.4_5.ORTP库的源码分析1_2

### 3.4.1、ORTP库概览

(1)库本身没有main，提供一堆功能函数，都在src目录下

(2)库的使用给了案例，有main，在src/tests目录下

(3)相关数据结构和头文件在include/ortp目录下

(4)ortp实现了rtp和rtcp协议，前者负责传输，后者负责控制和同步协调

### 3.4.2、ORTP库的使用案例

(1)src/tests/rtpsend.c

(2)ortp_init及av_profile_init

(3)ortp_scheduler_init和ORTP调度器：一个任务中完成多个会话的发送和接收，类似于select

(4)rtp_session_new和rtp的会话管理

### 3.4.3、rtp的session

(1)rtp通过会话来管理数据发送和接收，会话的本质是一个结构体，管理很多信息

(2)创建会话用rtp_session_new

(3)rtp发送用rtp_session_send_with_ts

(4)底层真正干活的还是socket接口那一套，参考rtpsession_inet.c

### 3.4.4、ORTP的一些小细节

(1)port.c中对OS的常用机制（任务创建和销毁、进程管理和信号量等）进行了封装，便于将ortp移植到不同平台中

(2)utils.c中实现了一个双向链表

(3)str_util.c中实现了一个队列管理

(4)rtpparse.c和rtcpparse.c文件实现了解析收到的rtp数据包的部分

(5)jitterctl.c中实现了jitter buffer来防抖。jitter buffer技术是ip 音视频通信里相对比较高级的主题，jitter buffer模块好坏通常是衡量一个voip客户端/服务器好坏的技术点之一，尤其是在网络抖动比较严重，如3g, wifi环境，数据包的rtt值不均衡往往会导致语音卡顿，丢字等现象，jitter buffer 模块通过缓存一段数据包，把数据包重排，并均匀的送给播放端，一个好的jitter buffer实现通长是动态调整缓存大小的，在网络延迟大，抖动严重时会动态增加缓存大小，在网络恢复时动态减小缓存大小以减少端到端的播放延迟。

 

 

## 3.4.6_7.RTP发送实验源码分析1_2

### 3.4.6.1、ortp库相应API

### 3.4.6.2、发送函数的重点讲解

### 3.4.6.3、可能的拓展方向

(1)裁剪sample到最简化

(2)修改一些参数做实验（譬如每包字节数、IP地址、端口号等）

 

buffer[pos-2]=(NALU & 0x60)|28;

FU header:  1字节的分片单元头

   +---------------+

   |0|1|2|3|4|5|6|7|

   +-+-+-+-+-+-+-+

   |S|E|R|  Type  |

   +---------------+

S: (1 bit)

  当设置成1，开始位指示分片NAL单元的开始。当跟随的FU荷载不是分片NAL单元荷载的开始，开始位设为0。

E: (1 bit)

  当设置成1，结束位指示分片NAL单元的结束，即荷载的最后字节也是分片NAL单元的最后一个字节。

  当跟随的FU荷载不是分片NAL单元的最后分片,结束位设置为0。

R: (1 bit)

  保留位必须设置为0，接收者必须忽略该位。

Type: (5 bit)

​    NAL单元荷载类型定义。

FU payload : 分片单元荷载。

————————————————

版权声明：本文为CSDN博主「chenchong_219」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：https://blog.csdn.net/chenchong_219/article/details/37996161

 

  buffer[pos-1]=(NALU & 0x1f);

参考 ：https://blog.csdn.net/evsqiezi/article/details/8492593

参考：

https://blog.csdn.net/pds574834424/article/details/78150474?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase

参考：https://www.cnblogs.com/likwo/p/3532856.html

## 3.4.8.VLC的sdp文件解析和课程总结

### 3.4.8.1、SDP文件格式

### 3.4.8.2、第三季总结  

技术娱乐思维

 

 

 

 

 

 

 

 

 

 

 

 

# 第四章 图像sensor的特性和驱动解析

## 4.0 章节概要

**4.1.本季课程主要内容和安排**

  本节讲解课程的总体安排，和官方文档中关于sensor接口的描述部分。

**4.2.更换OV9712并且做配置更改和测试**

  本节在开发板上更换OV9712，并且做必要的配置更改，运行各个版本测试程序。

**4.3.MIPI和LVDS和并口的细节讲解1**

  本节讲解sensor的各种接口，本节主要讲了并口。

**4.4.MIPI和LVDS和并口的细节讲解2**

  本节接着讲解sensor的各种接口，本节主要讲了LVDS和MIPI CSI。

**4.5.HI3518E的Sensor接口引脚复用设置1**

  本节讲解sensor接口的引脚复用设置方法，主要是查引脚复用图。

**4.6.HI3518E的Sensor接口引脚复用设置2**

  本节继续讲解sensor接口引脚复用设置方法，主要是寄存器的设置。

**4.7.sensor驱动源码解析1**

  本节开始讲解sensor驱动的源码，主要是从框架结构出发讲解sensor驱动的架构。

**4.8.sensor驱动源码解析2**

  本节继续讲解sensor驱动源码，主要深入2个c文件中讲解sensor驱动的细节层次。

**4.9.ISP_3A框架解读**

  本节讲解ISP 3A手册，该手册是我们理解海思3A框架的关键，而sensor和isp就在这里。

**4.10.sensor驱动编译实战**

  本节首先回顾sensor驱动架构，然后以黑电平为案例，通过修改驱动重新编译实战来体验sensor驱动如何修改、提升实战能力。

**4.11.sensor驱动的寄存器操作**

  

 

本节详解sensor驱动中和sensor寄存器设置有关的部分，并且以flip和mirror的设置为案例来演示如何查手册确定值并修改验证。

**4.12.sensor驱动部分贯通总结**

  本节对sensor驱动的整个部分做总结，结合整个课程学到的内容，提升大家对sensor本身的掌握和海思mpp体系中sensor、3A、isp相关部分理解。

 

## 4.1.本季课程主要内容和安排

### 4.1.1、本课程主要内容

(1)查看SDK中相应文档，重点是SoC对Sensor的支持

(2)更换另一个Sensor（OV9712），并实现之前的实验

(3)Sensor接口：并口/LVDS/MIPI CSI

(4)SoC的Sensor接口引脚复用设置

(5)sensor驱动源码详解

(6)sample中sensor相关的部分详解

### 4.1.2、查看SDK的2个相关文档

**视频接口**

输入:

\- 支持8/10/12/14 bit RGB Bayer/ RGB-IR输入，时钟
 频率最高100MHz
 \- 支持BT.601、 BT.656、 BT.1120
 \- 支持4 x Lane MIPI/Hispi/LVDS接口.
 \- 支持与SONY、 Aptina、 OmniVision、 Panasonic等
 主流高清CMOS对接.
 \- 兼容多种sensor电平
 \- 提供可编程sensor时钟输出
 \- 支持输入最大分辨率为2M (1920*1080) Pixel

 

 

## 4.2.更换OV9712并且做配置更改和测试

### 4.2.1、更改配置脚本

在开发板中的 /etc/profile 中将加载驱动程序的脚本摄像头参数改位ov9712:

./load3518e -i -sensor ov9712 -osmem 32 -total 64

###    4.2.2、运行rtsp传输的测试版本

 

 

 

 

 

 

  出现I2C_WRITE error! 是正常的。

### 4.2.3、运行官方SDK sample的测试版本

### 4.2.4、运行ORTP传输的测试版本

  ./sample_ov9712_venc_4_ORTP 0 

### 4.2.5、更换sensor的总结

(1)程序框架做好多种sensor支持的框架

(2)注意硬件接线上面的差异

(3)加载不同的驱动，做不同的参数设置

 

 

## 4.3_4.MIPI和LVDS和并口的细节讲解1_2

### 4.3.1、并口Sensor

(1)OV9712和AR0130都是并口的

(2)并口的接口定义：参考AR0130的原理图pdf

 

(3)并口传输的是CMOS电平信号（重点是非差分）

   (4)并口sensor属于较低端老旧的，新型高像素的都是MIPI/LVDS/HISPI等差分信号的

因为数据线有12根，数据线有多根的我们就叫他并口

### 4.3.2、LVDS

   (1)low voltage differential signal，低电压差分信号

(2)接口由1组差分clock和若干组差分信号线组成

(3)LVDS主要用于视频传输的2个领域：camera和主控、LCD和主控

(4)LVDS利用差分抗干扰能力，提升clock频率从而提升带宽，传输距离也更远

(5)LVDS的数据线组数越多带宽越大、clock频率越高带宽越大（牺牲抗干扰和距离）

(6)并口和LVDS之间可以互转，但是需要专门的电平转换芯片（类似于232和485）

LVDS没有I2C控制

 

参考： https://blog.csdn.net/perfect1t/article/details/81627066

 

### 4.3.3、MIPI（MIPI-CSI2）

(1)MIPI: mobile industry processor interface，移动工业处理器接口

(2)MIPI接口由1组差分clock和1-4组差分信号线组成

(3)MIPI和LVDS虽然都是差分对信号，但是不兼容，不能直接对接

(4)MIPI的架构层次更分明，广泛应用在手机平板等领域中，**可以认为MIPI是LVDS的升级版**

(5)MIPI的数据线组数越多带宽越大、clock频率越高带宽越大（牺牲抗干扰和距离）

(6)MIPI和LVDS和并口之间均可以互相转换，但是需要专门的电平转换芯片

主流sensor接口都是用MIPI的， MIPI专门位手机设计的，目的简化手机的设计；

### 4.3.4、总结

(1)老旧的、低端的、数据量小的就用电平信号；新的、高端的、数据量大的都用差分信号。

(2)要通信，物理层、协议层、应用层都得能对接才行。

(3)因为历史原因，很多行业会使用不同的接口标准，必要时需要去互相转换。

 

 

## 4.5_6.HI3518E的Sensor接口引脚复用设置1_2

### 4.5.1、查看引脚定义框图

 Sensor 的原理图里找到相应的引脚编号，然后再去Soc硬件原理图找查找相应的编号所对应的引脚，然后查询SoC使用手册找到相应的寄存器设置；

 

 

 

### 4.5.2、找到相应设置寄存器

### 4.5.3、himm工具

  帮我们读写海思芯片的寄存器。

例如：himm 0x200f007c 0x1;       # VI_DATA13

  表示将 0x200f007c 这个地址的寄存器的值置为0x1;

## 4.7.sensor驱动源码解析1

### 4.7.1、sensor驱动源码寻找

(1)从sample入手

(2)sensor层驱动在component/isp中（mpp\component\isp\sensor\ar0130）

(3)底层i2c驱动在kernel中

### 4.7.2、sensor驱动的框架

(1)mpp定义了一套sensor驱动的实现和封装

(2)xxxx_cmos.c中定义回调和上层函数（应用层驱动，面向功能）

(3)xxxx_sensor_ctl.c中定义底层硬件相关的寄存器值配置函数(应用层驱动，面向寄存器)。

(4)kernel中的I2C驱动提供i2c层面的物理层操作接口

### 4.7.3、什么是回调函数

你到一个商店买东西，刚好你要的东西没有货，于是你在店员那里留下了你的电话，过了几天店里有货了，店员就打了你的电话，然后你接到电话后就到店里去取了货。在这个例子里，你的电话号码就叫回调函数，你把电话留给店员就叫登记回调函数，店里后来有货了叫做触发了回调关联的事件，店员给你打电话叫做调用回调函数，你到店里去取货叫做响应回调事件。回答完毕。 0

 

**问题1** ：那为什么不引入头文件，直接调用函数呢？就相当于知道了你家的门牌号码，直接找你就是了。

因為店員在設計出來的時候並不知道誰回来

**问题2**：跟硬件中断的区别？

硬件中断是一种更广义上的回调函数……

回调就是「当发生 A 事件的时候调用 fA () 函数」，中断不就是这样吗。
 但一般说回调的时候，都在说比较上层的东西，中断这玩意太底层了，思想是回调的思想，但一般不用「中断回调」「硬件回调」这样的词。

其實調用回調函數的本質上來講也是觸發一個中斷的意思。

 

\--------------------------------------------------------

 

作者：常溪玲
 链接：https://www.zhihu.com/question/19801131/answer/13005983 


 来源：知乎
 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

 

编程分为两类：系统编程（system programming）和应用编程（application programming）。所谓系统编程，简单来说，就是编写库；而应用编程就是利用写好的各种库来编写具某种功用的程序，也就是**应用**。系统程序员会给自己写的库留下一些接口，即API（application programming interface，应用编程接口），以供应用程序员使用。所以在抽象层的图示里，库位于应用的底下。

当程序跑起来时，一般情况下，应用程序（application program）会时常通过API调用库里所预先备好的函数。但是有些库函数（library function）却要求应用先传给它一个函数，好在合适的时候调用，以完成目标任务。这个被传入的、后又被调用的函数就称为**回调函数**（callback function）。

打个比方，有一家旅馆提供叫醒服务，但是要求旅客自己决定叫醒的方法。可以是打客房电话，也可以是派服务员去敲门，睡得死怕耽误事的，还可以要求往自己头上浇盆水。这里，“叫醒”这个行为是旅馆提供的，相当于库函数，但是叫醒的方式是由旅客决定并告诉旅馆的，也就是回调函数。而旅客告诉旅馆怎么叫醒自己的动作，也就是把回调函数传入库函数的动作，称为**登记回调函数**（to register a callback function）。如下图所示（图片来源：维基百科）：

 

可以看到，回调函数通常和应用处于同一抽象层（因为传入什么样的回调函数是在应用级别决定的）。而回调就成了一个高层调用底层，底层再回过头来调用高层的过程。（我认为）这应该是回调最早的应用之处，也是其得名如此的原因。

**回调机制的优势**

从上面的例子可以看出，回调机制提供了非常大的灵活性。请注意，从现在开始，我们把图中的库函数改称为中间函数了，这是因为回调并不仅仅用在应用和库之间。任何时候，只要想获得类似于上面情况的灵活性，都可以利用回调。

这种灵活性是怎么实现的呢？乍看起来，回调似乎只是函数间的调用，但仔细一琢磨，可以发现两者之间的一个关键的不同：在回调中，我们利用某种方式，把回调函数像参数一样传入中间函数。可以这么理解，在传入一个回调函数之前，中间函数是不完整的。换句话说，程序可以在运行时，通过登记不同的回调函数，来决定、改变中间函数的行为。这就比简单的函数调用要灵活太多了。
 `even.py`

 \#回调函数1

\#生成一个2k形式的偶数

def double(x):

  return x * 2

  

\#回调函数2

\#生成一个4k形式的偶数

def quadruple(x):

  return x * 4

 

 

 

```
callback_demo.py
```

from even import *

 

\#中间函数

\#接受一个生成偶数的函数作为参数

\#返回一个奇数

def getOddNumber(k, getEvenNumber):

  return 1 + getEvenNumber(k)

  

\#起始函数，这里是程序的主函数

def main():  

  k = 1

  \#当需要生成一个2k+1形式的奇数时

  i = getOddNumber(k, double)

  print(i)

  \#当需要一个4k+1形式的奇数时

  i = getOddNumber(k, quadruple)

  print(i)

  \#当需要一个8k+1形式的奇数时

  i = getOddNumber(k, lambda x: x * 8)

  print(i)

  

if __name__ == "__main__":

  main()

运行`callback_demp.py`，输出如下：

3

5

9

上面的代码里，给`getOddNumber`传入不同的回调函数，它的表现也不同，这就是回调机制的优势所在。值得一提的是，上面的第三个回调函数是一个匿名函数。

易被忽略的第三方

通过上面的论述可知，中间函数和回调函数是回调的两个必要部分，不过人们往往忽略了回调里的第三位要角，就是中间函数的调用者。绝大多数情况下，这个调用者可以和程序的主函数等同起来，但为了表示区别，我这里把它称为起始函数（如上面的代码中注释所示）。

 

 

 

之所以特意强调这个第三方，是因为我在网上读相关文章时得到一种印象，很多人把它简单地理解为两个个体之间的来回调用。譬如，很多中文网页在解释“回调”（callback）时，都会提到这么一句话：“If you call me, I will call you back.”我没有查到这句英文的出处。我个人揣测，很多人把起始函数和回调函数看作为一体，大概有两个原因：第一，可能是“回调”这一名字的误导；第二，给中间函数传入什么样的回调函数，是在起始函数里决定的。实际上，回调并不是“你我”两方的互动，而是ABC的三方联动。有了这个清楚的概念，在自己的代码里实现回调时才不容易混淆出错。

另外，回调实际上有两种：阻塞式回调和延迟式回调。两者的区别在于：阻塞式回调里，回调函数的调用一定发生在起始函数返回之前；而延迟式回调里，回调函数的调用有可能是在起始函数返回之后。这里不打算对这两个概率做更深入的讨论，之所以把它们提出来，也是为了说明强调起始函数的重要性。网上的很多文章，提到这两个概念时，只是笼统地说阻塞式回调发生在主调函数返回之前，却没有明确这个主调函数到底是起始函数还是中间函数，不免让人糊涂，所以这里特意说明一下。另外还请注意，本文中所举的示例均为阻塞式回调。延迟式回调通常牵扯到多线程，我自己还没有完全搞明白，所以这里就不多说了。


 \--------------------------------------------------------------------
 作者：no.body
 链接：https://www.zhihu.com/question/19801131/answer/27459821
 来源：知乎
 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

 

 

## 4.8.sensor驱动源码解析2

### 4.8.1、xxxx_cmos.c中实现和注册回调

  int sensor_register_callback(void) 这函数来提供回调函数的一个实体。

 

HI_S32 HI_MPI_ISP_SensorRegCallBack(ISP_DEV IspDev, SENSOR_ID SensorId, \

ISP_SENSOR_REGISTER_S *pstRegister)

 

这个函数用来注册回调函数的。 

IspDev = 0 ，因为3518E只有一个VI；

 

SENSOR_ID :  senser ID，感光元件的编号；

ISP_SENSOR_REGISTER_S ： 这个结构里面都是函数指针，定义了sensor的行为和操作。

 

typedef struct hiISP_SENSOR_REGISTER_S

{

  ISP_SENSOR_EXP_FUNC_S stSnsExp;

} ISP_SENSOR_REGISTER_S;

 

typedef struct hiISP_SENSOR_EXP_FUNC_S

{

  HI_VOID(*pfn_cmos_sensor_init)(HI_VOID); 

  HI_VOID(*pfn_cmos_sensor_exit)(HI_VOID);

  HI_VOID(*pfn_cmos_sensor_global_init)(HI_VOID);

  HI_S32(*pfn_cmos_set_image_mode)(ISP_CMOS_SENSOR_IMAGE_MODE_S *pstSensorImageMode);

  HI_VOID(*pfn_cmos_set_wdr_mode)(HI_U8 u8Mode);

 

  /* the algs get data which is associated with sensor, except 3a */

  HI_U32(*pfn_cmos_get_isp_default)(ISP_CMOS_DEFAULT_S *pstDef);

  HI_U32(*pfn_cmos_get_isp_black_level)(ISP_CMOS_BLACK_LEVEL_S *pstBlackLevel);

  HI_U32(*pfn_cmos_get_sns_reg_info)(ISP_SNS_REGS_INFO_S *pstSnsRegsInfo);

 

  /* the function of sensor set pixel detect */

  HI_VOID(*pfn_cmos_set_pixel_detect)(HI_BOOL bEnable);

 

} ISP_SENSOR_EXP_FUNC_S;

 

\----------------------------------------------------------------

 

这个结构中的函数肯定在之前就被填充好了，就在 HI_MPI_ISP_SensorRegCallBack这个函数的前一句 cmos_init_sensor_exp_function(&stIspRegister.stSnsExp);这个函数中填充的；

 

//file in : mpp/component/isp/sensor/ar0130/ar0130_cmos.c

 

HI_S32 cmos_init_sensor_exp_function(ISP_SENSOR_EXP_FUNC_S *pstSensorExpFunc)

{

  memset(pstSensorExpFunc, 0, sizeof(ISP_SENSOR_EXP_FUNC_S));

 

  pstSensorExpFunc->pfn_cmos_sensor_init = sensor_init;

  

pstSensorExpFunc->pfn_cmos_sensor_exit = sensor_exit;

  pstSensorExpFunc->pfn_cmos_sensor_global_init = sensor_global_init;

  pstSensorExpFunc->pfn_cmos_set_image_mode = cmos_set_image_mode;

  pstSensorExpFunc->pfn_cmos_set_wdr_mode = cmos_set_wdr_mode;

  

  pstSensorExpFunc->pfn_cmos_get_isp_default = cmos_get_isp_default;

  pstSensorExpFunc->pfn_cmos_get_isp_black_level = cmos_get_isp_black_level;

  pstSensorExpFunc->pfn_cmos_set_pixel_detect = cmos_set_pixel_detect;

  pstSensorExpFunc->pfn_cmos_get_sns_reg_info = cmos_get_sns_regs_info;

 

  return 0;

}

 

//file in : /mpp/component/isp/sensor/ar0130/ar0130_sensor_ctl.c

void sensor_init()

{

 

  if (1 == gu8SensorImageMode)  /* SENSOR_720P_30FPS_MODE */

  {

​    sensor_init_720p_30fps();

​    bSensorInit = HI_TRUE;

  }

  else if (2 == gu8SensorImageMode) /* SENSOR_960P_30FPS_MODE */

  {

​    sensor_init_960p_30fps();

​    bSensorInit = HI_TRUE;

  }

  else

  {

​    printf("Not support this mode\n");

  }

  

}

 

 

void sensor_init_720p_30fps()

{

//[720p30]

 

  sensor_write_register( 0x301A, 0x0001 );  // RESET_REGISTER

  delay_ms(200); //ms

  sensor_write_register( 0x301A, 0x10D8 );  // RESET_REGISTER

  delay_ms(200); //ms

  //Linear Mode Setup

  //AR0130 Rev1 Linear sequencer load 8-2-2011

  sensor_write_register( 0x3088, 0x8000 );// SEQ_CTRL_PORT

  …

  …

  }

  这主要就是设置我们sensor的寄存器，sensor_write_register()函数里面主要就是通过sensor_i2c_init();函数初始化i2c 后用ret = ioctl(g_fd, I2C_16BIT_REG, 0)函数来改写3518E芯片 sensor相关寄存器值的设置。

除了绑定一些初始化sensor配置函数外还有 AE自动曝光，awb 自动白平衡，以及af 自动对焦的一些回调函数的绑定（这个产品没有自动对焦功能，所以没有af相关配置）。

。

### 4.8.2、xxxx_sensor_ctl.c中配置sensor寄存器

### 4.8.3、isp和3a的联系与区别是什么？

[Willis Zen](http://www.zhihu.com/people/lovelers)上善若水

[2 人赞同](http://www.zhihu.com/question/37697376)

你说的这个问题，不是很多人能够回答的，我也只能把我知道的告诉你。
 isp 是image signal processing，用于图像处理，比如gamma调整，**dynamic range correction，smmoth，sharpness，format convert，resize，edge enhancement ，color correction等操作都是这里完成的**。
 3aa 是**ae/af/awb algorithms**。 3aa 的硬件模块分析输出3a的统计信息。3aa软件是分析统计信息值进行算法处理。两者关系是，数据经过3aa处理后，传递isp进行图像处理，3aa 算法的部分动作是需要isp硬件完成的。

 

3A:
 AE/AF/AWB 。对应的是相机的功能，即自动曝光，自动对焦和自动白平衡。三个功能的实现都需要软硬件结合起来实现。但基本都是一个负反馈闭环控制系统，即状态硬件模块输出亮度值/FV值/灰度空间，然后对应的3A算法进行计算反馈到相应的处理单元做处理，知道相应的值稳定下来。

 

ISP：Image Signal Processor ，即图像信号处理器，形式上指一个硬件，和CPU类似。但现在ISP都是泛指，因为相机硬件中很多单独硬件模块组成，如做色彩转换的，做滤波降噪的，有做裁剪，做旋转的。

 

## 4.9.ISP_3A框架解读

1.概述

Hi3518EV200_ISP_3A 版本依赖于相应的 SDK 大版本，通过一系列数字图像处理算法完成对数字图像的效果处理。主要包含 Firmware 框架及海思 3A 库， Firmware 提供算法的基本框架，处理统计信息，驱动数字图像处理算法，并包含坏点校正、去噪、色彩增强、镜头阴影校正等处理。 3A 库以注册的方式，添加到 Firmware 中，完成曝光、白平衡、色彩还原等处理。

2.设计思路

   ISP 的 Firmware 包含三部分，一部分是 ISP 控制单元和基础算法单元，即 ISP 库，一部分是 AE/AWB/AF 算法库，一部分是 sensor 库。 Firmware 设计的基本思想是单独提供 3A 算法库，由 ISP 控制单元调度基础算法单元和 3A 算法库，同时 sensor 库分别向ISP 库和 3A 算法库注册函数回调，以实现差异化的 sensor 适配。 ISP firmware 设计思路如图 1-1 所示。

不同的 sensor 都向 ISP 库和 3A 算法库注册控制函数，这些函数都以回调函数的形式存在。 ISP 控制单元调度基础算法单元和 3A 算法库时，将通过这些回调函数获取初始化参数，并控制 sensor，如调节曝光时间、模拟增益、数字增益，控制 lens 步进聚焦或旋转光圈等。
 图1-1 ISP firmware 设计思路

主要的参考资料参考: [ISP_3A开发指南.pdf](file:///C:/Users/administer/AppData/Roaming/Microsoft/Word/ISP_3A开发指南.pdf)

## 4.10.sensor驱动编译实战

### 4.10.1、sensor的注册接口分析

### 4.10.2、黑电平

光通过镜头投到sensor上,sensor将光信号转为模拟电信号,模拟电信号通过AD转换成数字信号。数字信号的大小决定了该颜色的亮度，数字为零表示颜色为全黑，那么在sensor那部分，对应颜色位全黑色的电平的大小表示位黑电平。

  黑电平是可以调的，全黑是一个相对的概念，可以根据人的感官来来调节。

 

 

### 4.10.3、sensor驱动编译实战

(1)修改驱动源码

(2)清除，并重新编译

(3)确认mpp中lib目录下的libsnsxxx.a/so已经被更新

(4)重新编译sample并运行查看效果

  

## 4.11.sensor驱动的寄存器操作

(1)sensor内部有若干寄存器，可以通过I2C接口来读写 

(2)数据手册有对寄存器的基本说明

(3)经验：大部分寄存器设置厂家会给，偶尔需要自己调一些

### 4.11.1、实战任务：修改sensor的flip和mirror寄存器查看效果

  修改驱动mpp/component/isp/sensor/ar0130/ar0130_sensor_ctl.c中的

void sensor_init_720p_30fps()

  添加一句：

  //set filp

  sensor_write_register( 0x3040, 0x8000);

(1)查sensor数据手册的寄存器列表 AR0101_PR_C.pfd 搜索flip关键词；

(2)改sensor驱动代码

  修改驱动mpp/component/isp/sensor/ar0130/ar0130_sensor_ctl.c中的

void sensor_init_720p_30fps()

  添加一句：

  //set filp

  sensor_write_register( 0x3040, 0x8000);

 

 

 

 

(3)**重新编译isp 这步骤是必须的， 如果你只编译sample/venc/中的makefile 是没有反应的，因为这个makefile只负责链接相关的.o文件。而且在venc中因为只链接，所以要make clean之后再make；**

(4)确认sensor库已经更新到mpp中

(5)重新编译sample，运行测试

## 4.12.sensor驱动部分贯通总结

### 4.12.1、体系思想

(1)成熟的商业解决方案都是一整套设计好的模式

(2)第一步是理解，第二步是用起来，第三步是小修改，第四步是大修改，第五步是创造

### 4.12.2、海思方案的sensor驱动相关体系关键点

(1)搞清楚sensor的本质：光电转换+AD+ISP+并口/MIPI/LVDS

(2)ISP有多种实现：sensor内置、主SoC内置、外接专用ISP芯片

(3)3A是为最终图像效果负责的，3A的实现有赖于镜头、sensor、isp等各部门协同工作

(4)海思的体系中把sensor和3A、ISP实现为：指针挂接注册的各自独立模块

### 4.12.3、扩展学习方向

(1)sensor的各种参数

(2)其他几种sensor的驱动和对比实现

(3)isp的firmware

(4)3A算法相关知识

 

 

 

 

 

 

# 第五章 海思平台上USB WIFI的移植与局域网无线调试和视频流预览

## 5.0 章节概要

**5.1.海思平台上USB WIFI移植概述**

  本节讲解本季课程的起点和先前相关的课程，让大家对USB WIFI移植有个计划。

**5.2.AP模式USB WIFI驱动移植**

  本节带大家手把手移植USB WIFI在AP模式下的驱动源码。

**5.3.AP模式USB WIFI传输视频实战**

  本节手把手带大家部署USB WIFI，并且做好设置，使用Windows连接开发板做视频传输。

**5.4.USB WIFI做sta模式的驱动移植和部署**

  本节带大家手把手移植USB WIFI在sta模式下的驱动源码。

**5.5.移植wpa_supplicant**

  本节带大家手把手移植wpa_supplicant以及openssl，这个是linux下管理wifi的利器。

**5.6.USB WIFI做sta模式的ORTP视频传输实验**

  本节手把手带大家部署sta模式的USB WIFI，并且连接路由器，使用Windows连接开发板做视频传输。

**5.7.纯WIFI无线调试环境的搭建**

  本节将虚拟机ubuntu也拉进来，实现开发板、windows、虚拟机ubuntu三者作为sta连接路由器，并且做好设置，开机自动无线调试。

 

 

 

 

 

 

## 5.1.海思平台上USB WIFI移植概述

### 5.1.1、移植WIFI背景

(1)必要性：家用camera很多需要wifi联网功能

(2)WIFI接口：SDIO or USB

### 5.1.2、移植的起点

(1)基于项目积木《USB WIFI网卡在X210上的移植》课程，已包含进本季课程中

(2)基于USB WIFI网卡官方移植匹配后的驱动来移植 

(3)基于海思SDK中已经编译过的内核源码树来编译驱动

### 5.1.3、本季课程安排

(1)HI3518E+WIFI做AP，PC做sta，实现局域网内ortp视频传输

(2)HI2518E+WIFI做sta，PC做sta，外部路由器做AP，实现局域网内ortp视频传输

(3)HI2518E+WIFI做sta，PC做sta，外部路由器做AP，实现全程无线调试（彻底不用有线网卡）

 

## 5.2.AP模式USB WIFI驱动移植

### 5.2.1、源码

(1)复制到ubuntu中实验目录

(2)解压进入

 

 

 

 

### 5.2.2、修改移植

(1)include/rtmp_def.h中1627行左右，修改网络名始终为：wlan

(2)使用提供的Makefile替换掉原来的Makefile

(3)./os/linux/config.mk中添加EXTRA_FLAGS

(4)修改Makefile或者手工复制xx.ko到/home/aston/rootfs中

 

 

## 5.3.AP模式USB WIFI传输视频实战

### 5.3.1、部署USB WIFI驱动使之工作为AP

(1)先lsusb，然后插入USB WIFI模块，再lsusb，确认模块被识别了

(2)在开发板中部署 /etc/Wireless/RT2870AP/RT2870AP.dat，并修改以下几个配置项

   SSID=MT7601AP_WPA（这里的名字是在pc机上看到的连接ssid名）

   AuthMode=WPA2PSK（加密方式）

   EncrypType=TKIP;AES

   WPAPSK=1234567890（这个是密码）

(3)insmod xx.ko安装USB WIFI驱动模块

(4)ifconfig -a看是否有wlan0

(5)ifconfig wlan0 up, ifconfig wlan0 192.168.0.100

(6)ifconfig查看，确认wlan0工作正常

(7)用PC或手机的WIFI搜索看一下，是否有一个名为：MT7601AP_WPA的网络了，有了就成功了

(8)给PC强制分配静态IP

 

 

 

 

### 5.3.2、测试用例准备

(1)ORTP版本的sample，common/sample_common_venc.c中，修改LOCAL_HOST_IP为192.168.0.30

(2)make clean，make

(3)复制得到的sample_venc到/home/aston/rootfs中

### 5.3.3、测试实验

(1)开发板端：cp /mnt/sample_venc /home/  然后cd /home

(2)拔掉网线（此时也可以重启下开发板以彻底消除前面的影响）

(3)PC端WIFI连接 MT7601AP_WPA，输入密码 1234567890

(4)PC端设置无线网络的静态IP地址为：192.168.0.30

(5)PC端cmd去ping 192.168.0.100，或者反过来开发板端ping PC的无线IP

(6)开发板端执行 ./sample_venc

(7)PC端打开vlc播放器，使用第3季中的sdp配置，注意c=IN IP4 192.168.0.100，就能播放了。

注：

无线：

windows： 192.168.0.30

AP：    192.168.0.100

ubuntu：  192.168.0.50 （ubuntu的IP现在可以不管）

 

有线：

windows： 192.168.1.20

开发板：  192.168.1.10

ubuntu：  192.168.1.141

 

 

 

 

### 5.4.USB WIFI做sta模式的驱动移植和部署

### 5.4.1、源码修改

(1)用sta版本的driver，可以简单看一下区别

(2)修改makefile，参考AP模式下的修改点修改

(3)config.mk中修改，参考AP模式下的修改点修改

(4)修改网卡名称为wlan

(5)make clean && make

### 5.4.2、部署驱动

(1)部署/etc/Wireless/RT2870STA/RT2870STA.dat 

(2)部署ko

 

 

### 5.5.移植wpa_supplicant

### 5.5.1、源码下载

(1)wpa_supplicant   http://hostap.epitest.fi/wpa_supplicant/

(2)openssl       ftp://ftp.openssl.org/source/old/0.9.x/

下载openssl-0.9.8za.tar.gz

分别tar -zxvf解压wpa_supplicant-2.5.tar.gz和openssl-0.9.8za.tar.gz，

### 5.5.2、移植openssl

(1)给openssl打补丁。把wpa_supplicant-2.5里面的patches文件夹下的openssl-0.9.8za-tls-extensions.patch文件拷贝到openssl-0.9.8za目录下，运行：patch -p1 < openssl-0.9.8za-tls-extensions.patch

(2)修改Makefile

CC=arm-hisiv300-linux-gcc

AR=arm-hisiv300-linux-ar $(ARFLAGS) 

RANLIB=arm-hisiv300-linux-ranlib

 

INSTALLTOP = /tmp/openssl

OPENSSLDIR = /tmp/openssl

(3)make && make install

(4)如果报错

****Expected text after =item, not a number

****Expected text after =item, not a number

****Expected text after =item, not a number

****Expected text after =item, not a number

原因：OpenSSL 与 perl版本不兼容

解决方法：rm -f /usr/bin/pod2man 

### 5.5.3、移植wpa_supplicant

(1)解压并进入主目录

(2)cp /wpa_supplicant/defconfig .config

(3)修改.config，增加以下内容：

CC=arm-hisiv300-linux-gcc -L/tmp/openssl/lib

CFLAGS+=-I /tmp/openssl/include

LIBS+=-L/tmp/openssl/lib

(4)make   如果报错：

driver_nl80211.c:17:31: fatal error: netlink/genl/genl.h

进入wpa_supplicant目录下.config 

将CONFIG_DRIVER_NL80211=y 注释掉即可

(5)继续make，生成wpa_supplicant和wpa_cli后整个移植成功完成

 

 

 

### 5.6.USB WIFI做sta模式的ORTP视频传输实验

### 5.6.1、部署wpa_supplicant

(1)将wpa_supplicant、wpa_cli丢到开发板/usr/local/bin目录下

(2)创建配置文件/etc/wap_supplicant.conf，内容如下：

  ctrl_interface=/var/run/wpa_supplicant

 

  network={

  ssid="ZLSWLW-2"

  scan_ssid=1

  key_mgmt= WPA-EAP WPA-PSK IEEE8021X NONE

  pairwise=TKIP CCMP

  group=CCMP TKIP WEP104 WEP40

  psk="justdoit1234"

  }

### 5.6.2、sta模式的USB WIFI设置

(1)安装驱动 insmod mt7601Usta.ko

(2)设置静态IP地址

ifconfig wlan0 up, ifconfig wlan0 192.168.0.233

(3)开发板sta连接AP  （-dd打开调试信息）

wpa_supplicant -Dwext -iwlan0 -c/etc/wpa_supplicant.conf -dd &

(4)查看连接状态

wpa_cli -i wlan0 status   

(5)测试连接效果

ping 192.168.0.1   （网关AP）

route add default gw 192.168.0.1 dev wlan0

ping 8.8.8.8     （google的dnw server）

ping www.zhulaoshi.org

### 5.6.3、ORTP视频传输测试

(1)修改LOCAL_HOST_IP 为192.168.0.100（和我的Widnows的WIFI自动分配的IP一样）

(2)编译得到sample并到开发板运行

(3)修改sdp中c=IN IP4 192.168.0.232,打开sdp播放

注：

AP：    192.168.0.1

PC：    192.168.0.100

开发板：  192.168.0.233

ubuntu：  192.168.0.244

 

 

### 5.7.纯WIFI无线调试环境的搭建

### 5.7.1、原理

(1)UBUNTU和Windows桥接，这样ubuntu、windows、开发板三者都做sta，连同一个AP

(2)开发板事先移植部署好USB WIFI驱动、wpa_supplicant等

(3)开发板/etc/profile中设计好配置WIFI启动、连接AP、静态IP等

(4)开发板/etc/profile中设置让开发板通过无线mount虚拟机

### 5.7.2、实战

(1)先确认开发板部署好USB WIFI驱动、wpa_supplicant工具

(2)sample放到虚拟机ubuntu的共享文件夹中

(3)开发板/etc/profile中做好各种配置

(4)设置ubuntu桥接到windows的无线网卡，并给ubuntu设置静态IP地址

(5)开发板重启，看效果

 

\#ifconfig eth0 192.168.1.10

\#mount -t nfs -o nolock 192.168.0.103:/home/aston/rootfs /mnt

cd /ko      

./load3518e -i -sensor ar0130 -osmem 32 -total 64

​                      

insmod /home/mt7601Usta.ko            

ifconfig wlan0 up             

ifconfig wlan0 192.168.0.233        

wpa_supplicant -Dwext -iwlan0 -c/etc/wpa_supplicant.conf &

route add default gw 192.168.0.1 dev wlan0          

​                              

mount -t nfs -o nolock 192.168.0.244:/home/aston/rootfs /mnt

cd /mnt 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

# 第六章 RTSP协议详解与实时流视频预览

## 6.0 章节概要

 **这一章节需要重点掌握，基本上从业视频行业两年的大概也就掌握该季水平的样子。**

**6.1.本季核心技术点和讲法**

  本节首先演示基于rtsp传输的实验例程，然后讲述本季课程的重点内容点。

**6.2.H264编码原理和基本概念**

  本节从宏观角度讲解h264的编码原理和重要概念，包括I帧、P帧、宏块、片等。

**6.3.H264的NAL单元详解1**

  本节讲解SODB、RBSP、NALU等h264中重要概念，同时讲解常用的h264流分析工具软件，最后从宏观上讲了H264流的一些理解

**6.4.H264的NAL单元详解2**

  本节重点讲解H264的sequence、sps、pps、sei、NALU等，其重点是NALU的位定义和解析方法，这些是我们后续处理h264的rtsp传输的关键点。

**6.5.H264的NAL单元详解3**

  本节重点讲解sps和pps、sei的内部帧结构，以及h264的profile和level等重要概念。

**6.6.H264的NAL单元详解4**

  本节对h264的NAL部分进行总结，彻底打通前述概念，让大家对h264的整体码流内容和解析方法有本质的掌握。

**6.7.rtsp传输源码分析1**

  本节开始分析rtsp传输源码，主要是代码整体架构的分析。

**6.8.rtsp传输源码分析2**

  本节接着分析rtsp源码，主要是RtspServerListen相关的部分。

**6.9.rtsp传输源码分析3**

  本节接着分析rtsp源码，主要是主程序部分，以及和rtsp部分

**6.10.rtsp传输源码分析4**

  

 

本节接着分析rtsp源码，主要是client和server之间的命令和响应，以及rtsp通信建立的过程，关键字的parser等。

**6.11.rtsp传输实战分析**

  本节从实战角度分析rtsp传输代码，用实验验证的方式帮助我们进一步理解rtsp传输代码的工作流程。

**6.12.直接发送与环状buffer发送**

  本节讲述2种rtp包发送策略，直接发送和环状buffer发送，这2种方式我们提供的源码中都做了演示。

**6.13.rtsp分包发送h264的源码分析** 

  本节讲解h264分包的rtp发送部分代码，核心是分包后用fu_indicator和fu_header替代nalu的部分，以及相关的一些标志位等。

 

 

## 6.1.本季核心技术点和讲法

### 6.1.1、rtsp视频传输实验

   在做rtsp视频传输传输的时候，就必须了解h.264的帧结构

(1)调试ok的源码

(2)编译、运行、测试

### 6.1.2、源码简单浏览

Sample_rtsp.rar 

### 6.1.3、重点1：h.264帧结构

 

### 6.1.4、重点2：帧结构分析软件的使用

 

 

### 6.1.5、重点3：rtsp网络编程

 

### 6.1.6、重点4：wireshark网络抓包工具的使用

 

 

## 6.2.H264编码原理和基本概念

### 6.2.1、h.264编码原理

(1)图像冗余信息：空间冗余（在一幅图里面有很大块颜色是一样的）、时间冗余（视频上一帧和下一帧变化很少，有大量重复的颜色）。

(2)视频编码关键点：压缩比、算法复杂度、还原度(压缩之后还原后的视频图像和原版的相似度)。压缩分为软件压缩和硬件压缩，软件压缩用CPU来压缩，硬件是指利用DSP硬件单元来压缩。只有少数人才会做这个压缩，这个是非常复杂的工作，数学和计算机的专业知识需要非常高，可以说极高。我们这个级别研究的需要视频的传输以及到接收端解码这一过程。

(3)H.264的2大组成部分：VCL和NAL。VCL是如何进行视频压缩，里面存的就是视频压缩的算法。NAL视频流如何被网络传输，到接收端后如何进行解码播放。先研究NAL在研究VCL，如果你要先研究VCL你实现的方法都没有。

### 6.2.2、h.264编码相关的一些概念

(1)宏块 MB ： macroblock；多个像素组成的块，图像有些特点，在一部分区域有些颜色像素是非常相似的，我把一幅图片分成一块一块颜色相似的宏块。我们压缩不是以像素为单位而是以宏块为单位的。

(2)片 slice块： 比MB （宏块）大一个级别的，构成帧的一部分，一帧可能有一个slice或者多个slice；

(3)帧 frame ：以整张图像

(4)I帧：没有参考的帧，相当于起始帧；

B帧、P帧：有参考的帧，图像的内容是参考前一帧做出来的，因为图像有冗余B、P就是用来压缩的。时间压缩；

P帧只参考前面的，不参考后面的帧；

B帧不仅参考前面的，还参考后面的帧；

(5)帧率 fps 一秒钟有多少帧

(6)像素->宏块->片->帧->序列->码流

 

## 6.3.H264的NAL单元详解1

### 6.3.1、VCL和NAL的关系

(1)VCL只关心编码部分，重点在于编码算法以及在特定硬件平台的实现，VCL输出的是编码后的纯视频流信息，没有任何冗余头信息。

**(2)NAL关心的是VCL的输出纯视频流如何被表达和封包以利于网络传输**

**(3)SODB：String Of Data Bits    VCL部分输出的纯视频流**

**(4)RBSP：Raw Byte Sequence Payload**   

**(5)NALU：Network Abstraction Layer Units**

**(6)关系：**

   **SODB + RBSP trailing bits  = RBSP**

   **NAL header(1 byte) + RBSP   = NALU**

(7)总结：做编码器的人关心的是VCL部分；做视频传输和解码播放的人关心的是NAL部分。

### 6.3.2、H.264视频流分析工具

(1)雷神作品：SpecialVH264.exe

(2)国外工具：Elecard StreamEye Tools

(3)二进制工具：winhex

(4)网络抓包工具：wireshark

(5)播放器：vlc

### 6.3.3、h264视频流总体分析

(1)h264标准有多个版本，可能会有差异，具体差异不详

(2)网上看的资料有时讲法会有冲突，或者无法验证的差异

(3)我们的课程都是以海思平台为主、为准、为案例，不能保证其他平台也完全一样

(4)海思平台编码出来的H.264码流都是一个序列包含：1sps+1pps+1sei+1I帧+若干p帧。

 

## 6.4.H264的NAL单元详解2

### 6.4.1、相关概念

(1)序列 sequence

视频码流很长，需要把码流分为一小段一小段来处理。每一个sequence都有一个I帧。如果我们很长的时间里都只参考最开始的I帧，如果当中坏了一帧，后面的帧数据就全坏了。其实我们每一秒中的最开始的那一帧就是I真，一秒钟就是一个sequence；

(2)分隔符  二进制 00000001 ； 表示有新的片，可能是sps、pps、I帧、P帧。如果数据里面不能出现连续000000 ，怕和分隔符重复。需要在最右边的0改成3为000003。

(3)sps sequence parameter set 序列参数集 14Byte

(4)pps picture parameter set 图像参数集 4Byte

(5)sei supplemental enhancement information 补充增强信息5Byte

(6)NALU ***

分隔符后面紧跟的第一帧的第一个字节非常重要，

### 6.4.2、NALU详解

https://blog.csdn.net/jefry_xdz/article/details/8461343

**1、NAL全称Network Abstract Layer, 即网络抽象层。**

在H.264/AVC视频编码标准中，整个系统框架被分为了两个层面：视频编码层面（VCL）和网络抽象层面（NAL）。其中，前者负责有效表示视频数据的内容，而后者则负责格式化数据并提供头信息，以保证数据适合各种信道和存储介质上的传输。**因此我们平时的每帧数据就是一个NAL单元（SPS与PPS除外）**。在实际的H264数据帧中，往往帧前面带有00 00 00 01 或 00 00 01分隔符，一般来说编码器编出的首帧数据为PPS与SPS，接着为I帧……

 

如下图：

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

**2、如何判断帧类型（是图像参考帧还是I、P帧等）？**

 

 NALU类型是我们判断帧类型的利器，从官方文档中得出如下图：

  

 

 

 

 

 

 

 

 

 

 

 

 

我们还是接着看最上面图的码流对应的数据来层层分析，以00 00 00 01分割之后的下一个字节就是NALU类型，将其转为二进制数据后，解读顺序为从左往右算，如下:

（1）第1位禁止位，值为1表示语法出错

（2）第2~3位为参考级别

（3）第4~8为是nal单元类型

 

例如上面00000001后有67,68以及65

其中0x67的二进制码为：

0110 0111

4-8为00111，转为十进制7，参考第二幅图：7对应序列参数集SPS

 

其中0x68的二进制码为：

0110 1000

4-8为01000，转为十进制8，参考第二幅图：8对应图像参数集PPS

 

其中0x65的二进制码为：

0110 0101

4-8为00101，转为十进制5，参考第二幅图：5对应IDR图像中的片(I帧)

 

所以判断是否为I帧的算法为： （NALU类型 & 0001 1111） = 5  即  NALU类型 & 31 = 5

 

比如0x65 & 31 = 5

————————————————

版权声明：本文为CSDN博主「jefry_xdz」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：https://blog.csdn.net/jefry_xdz/article/details/8461343

## 6.5_6.H264的NAL单元详解3_4

### 6.5.1、sps和pps详解

https://www.cnblogs.com/wainiwann/p/7477794.html

### 6.5.2、H264的profile和level

https://blog.csdn.net/xiaojun111111/article/details/52090185

### 6.5.3、sequence

(1)一段h.264的码流其实就是多个sequence组成的

(2)每个sequence均有固定结构：1sps+1pps+1sei+1I帧+若干p帧

(3)sps和pps和sei描述该sequence的图像信息，这些信息有利于网络传输或解码

(4)I帧是关键，丢了I帧整个sequence就废了，每个sequence有且只有1个I帧

(5)p帧的个数等于fps-1

(6)I帧越大则P帧可以越小，反之I帧越小则P帧会越大

(7)I帧的大小取决于图像本身内容，和压缩算法的空间压缩部分

(8)P帧的大小取决于图像变化的剧烈程度

(9)CBR和VBR下P帧的大小策略会不同，CBR时P帧大小基本恒定，VBR时变化会比较剧烈

 

 

 

 

## 6.7.rtsp传输源码分析1

代码不是自己写的，从live555移植过来的。

 

## 6.8.rtsp传输源码分析2

1 RTSP 概述

RTSP（Real Time Streaming Protocol），参考标准为 RFC2326，RTSP 协议是基于文本的实时流传输协议，是 TCP/一个应用层协议。RTSP 在体系结构上位于 RTP （数据流）和 RTCP（控制流） 之上，其使用 TCP 或 UDP 完成数据的RTSP 相比，HTTP 请求由客户机发出    ，服务器作出响应，使用 RTSP 时，客户机和服务器都可RTSP 可以是双向的；RTSP 是用来控制声音或影像多媒体串流协议，并允许同时多个串流需求用的网络通信协定并不在其定义范围内。RTSP 协议默认端口：554，默认承载协议为 TCP。

从控制逻辑上来说 RTSP 和 FTP 相似，流控和数据流是分开的。

   2.图解

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

3、传输过程

  

 

 

 

 

 

 

 

 

 

 

 

RTSP 消息格式

1 请求格式（Request）

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

2 回应消息格式（Response）

  

 

 

 

 

 

 

 

 

3、RTSP 中的 C（Client）与 S（Server）交互流程图解

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

4、RTSP 关键字段说明

4.1 关键字：OPTIONS

得到服务器提供的可用方法（OPTION、DESCRIBE、SETUP、TEARDOWN、PLAY、PAUSE、SCALE、

GET_PARAMETER、SET_PARAMETER）。

4.2 关键字：DESCRIBE

请求流的 SDP 信息。

注解：此处需要了解 H264 Law Data 如何生成 SPS PPS 信息。

4.3 关键字：SETUP

客户端提醒服务器建立会话，并建立传输模式。

注解：此处确定了 RTP 传输交互式采用 TCP（面向连接）还是 UDP（无连接）模式。

4.4 关键字：PLAY

客户端发送播放请求。

注解：此处引入 RTP 协议及 RTCP 协议。

4.5关键字：PAUSE

播放暂停请求。

注解：此关键字经常用在录像回放当中，实时视频流几乎用不到。

4.6 关键字:TEARDOWN

客户端发送关闭请求

4.7 关键字:GET_PARAMETER

从服务器获取参数，目前主要获取时间参数（可扩展）

4.8 关键字：SET_PARAMETER

给指定的 URL 或者流设置参数（可扩展）

 

RTP 协议概述

RTP 全名是 Real-time Transport Protocol（实时传输协议）。它是 IETF 提出的一个标准，对应的 RFC 文档为 RFC3550 （RFC1889 为其过期版本） 。 RFC3550 不仅定义了 RTP， 而且定义了配套的相关协议 RTCP （Real-timeTransport Control Protocol，即实时传输控制协议）。RTP 用来为 IP 网上的语音、图像、传真等多种需要实时传输的多媒体数据提供端到端的实时传输服务。RTP 为 Internet 上端到端的实时传输提供时间信息和流同步，但并不保证服务质量，服务质量由 RTCP 来提供。RTP 用于在单播或多播网络中传送实时数据。

 

 

## 6.9.rtsp传输源码分析3

 

 

## 6.10.rtsp传输源码分析4

 

 

## 6.11.rtsp传输实战分析

 

 

## 6.12.直接发送与环状buffer发送

 

 

 

 

 

## 6.13.rtsp分包发送h264的源码分析

 

rtp_timestamp：

https://blog.csdn.net/jasonhwang/article/details/7316128

 

fu_indicator：

https://www.cnblogs.com/yjg2014/p/6144977.html

 

 

函数调用框架：

main

  RtspServer_init

​    RtspServerListen

​      RtspClientMsg

​       ParseRequestString

​    vdRTPSendThread

​      VENC_Sent

  SAMPLE_VENC_720P_CLASSIC

​    SAMPLE_COMM_VENC_Sentjin

 

 

 

 

ENC_Sent

 

 

 

 

# 第七章 视频打包为MP4格式并存储到TF卡的实现

## 7.1.MP4文件格式解析1

### 7.1.1、视频文件总体介绍

(1)视频文件的本质：记录压缩后的视频帧并且能被播放器还原解码播放

(2)视频文件的关键：高效率记录信息、兼容性

(3)视频文件的信息：索引信息、有效信息

### 7.1.2、MP4格式总体介绍

(1)MP4由MP3升级而来，包含video和audio在内

(2)MP4是h.264的最主流打包格式

(3)MP4文件内部采用网络字节序（大端模式） 

### 7.1.3、MP4学习路线

(1)学习MP4的组织形式和box解析

(2)移植和使用mp4v2开源库来打包MP4

(3)进一步研究MP4解包播放和mp4v2源码

(4)自己编程进行MP4的打包、解包、分割等。

### 7.1.4、正式开始MP4的组织形式的学习

 

## 7.2.MP4文件格式解析2

(1)整个MP4文件由若干个各种不同的box组成，打包和解包时都是以box为单位的

(2)MP4中有且只有一个ftyp box，该box位于整个MP4的开头位置

(3) box的头储存的是这个box的size,1表示这个size太大,`

 

## 7.3.MP4Info工具使用

 

## 7.4.mp4v2移植和播放实战1

### 7.4.1、下载mp4v2

(1)https://launchpad.net/ubuntu/+source/mp4v2/2.0.0~dfsg0-6

(2)解压，并在目录内创建_install目录作为安装目录

### 7.4.2、配置并编译

(1)sudo PATH=$PATH:/opt/hisi-linux/x86-arm/arm-hisiv300-linux/target/bin CC=arm-hisiv300-linux-gcc CXX=arm-hisiv300-linux-gcc ./configure --host=arm-hisiv300-linux --prefix=/home/aston/sambashare/mp4v2-2.0.0/_install --disable-option-checking --disable-debug --disable-optimize --disable-fvisibility --disable-gch --disable-largefile --disable-util --disable-dependency-tracking --disable-libtool-lock

(2)make

(3)make install

(4)检查各必要文件

### 7.4.3、部署

(1)生成的lib加到mpp lib里面

(2)include下文件添加到mpp/include中去

(3)lib/*so*加到开发板lib目录下

### 7.4.4、编译sample

(1)用提供的sample替换掉原来的sample

(2)make

 

 

## 7.5.mp4v2移植和播放实战2

### 7.5.1、准备TF卡

(1)TF卡格式化为FAT32文件系统，若失败可试试低层格式化软件如SDFormat之类

(2)开机后将TF卡进行挂载 mount -t vfat /dev/mmcblk0p1 /usr/mmc

(3)先检测下TF卡可用

### 7.5.2、运行和测试

(1)运行sample_venc，用rtsp测试确认有图像

(2)终止程序，取出TF卡用读卡器接电脑查看

 

 

## 7.6.MP4打包源码解析

 

 

 

 

 

 

 

 

## 7.7.mp4v2结合MP4Info学习分析

### 7.7.1、思路

(1)修改MP4打包源码，用MP4Info查看录制的MP4细节

(2)再深度：修改mp4v2源码中细节，再打包查看

### 7.7.2、实践1：去掉sps

### 7.7.3、实践2：去掉pps

 

 

## 7.8.添加网络telnet调试

### 7.8.1、为什么添加telnet调试

(1)linux系统的用户界面就是commandline，本质上由busybox提供

(2)busybox的命令行只有1个，一旦前台被占用就无法做其他操作

(3)解决方案有2个：一个是建立多个commandline，一个是开放其他用户界面。

### 7.8.2、telnet调试的原理

(1)在开发板中提前运行telnetd

(2)远程通过telnet的client连接server，构建一个用户界面

(3)这是非常传统典型的远程登录的方式···其实用过的

### 7.8.3、在HI3518E开发板上telnet远程登录调试实战

(1)命令行执行telnetd &，然后Windows打开CRT配置SSH至192.168.1.10开发板网口

(2)输入ssid：root，password：直接回车，进入

(3)问题：若遇到不断重启，将etc/profile 中的加载项移至/etc/init.d中的rcS文件中。这是因为因为开启telnet服务会多次加载profile

(4)可以将telnetd &加入rcS中开机默认加载

 

 

## 7.9.海思proc文件系统调试接口

### 7.9.1、proc文件系统的原理

### 7.9.2、海思proc文件系统调试的文档说明

### 7.9.3、额外提供的调试经验文档

 

 

 

 

 

 

 

 

 

 

 

 

# 第八章 海思平台OSD的实现

## 8.0章节概要

**8.1.海思平台OSD理论学习1**

  本节讲解OSD相关的理论，主要以海思MPP手册为准。

**8.2.海思平台OSD理论学习2**

  本节接着讲解OSD的相关理论。

**8.3.OSD实验演示和代码框架分析**

  本节演示OSD部分实验，并且对demo的源码架构进行整体分析。

**8.4.OSD代码实现分析1**

  本节分析OSD的代码部分，主要是RGN的建立模块。

**8.5.OSD代码实现分析2-前景和背景透明**

  本节继续分析OSD源码部分，主要是前景和背景透明等模块。

**8.6.OSD代码实现分析3-RGN内容填充**

  本节接着分析OSD源码部分，主要是rgn的内容填充部分。

**8.7.OSD代码实现分析4-RGN内容填充**

  本节接着分析OSD源码部分，主要是rgn的位图填充部分。

**8.8.OSD代码实现分析5-动态刷新BMP**

  本节分析OSD的代码部分，主要是BMP图片的动态刷新功能的实现。

**8.9.字库字符实现OSD**

  本节开始讲解用字库方式实现OSD显示的代码模块。

**8.10.字库字符实现OSD源码解析1**

  本节开始讲解字库方式实现OSD的源码分析，主要是OSD信息解析部分的分析。

**8.11.字库字符实现OSD源码解析2**

  本节接着讲解字库方式实现OSD的源码分析，主要是time和title的刷新显示部分。

**8.12.字库字符实现OSD源码解析3**

  本节接着讲解字库方式实现OSD的源码分析，主要是字库生成BMP图片的部分。

**8.13.字库字符实现OSD源码解析4**

  本节接着讲解字库方式实现OSD的源码分析，主要是上层代码框架的实现和细节。

 

## 8.1.海思平台OSD理论学习1

### 8.1.1、OSD概述

### 8.1.2、海思OSD的4种类型

### 8.1.3、4种OSD类型各自支持的模块和功能

### 8.1.4、海思OSD的几个重要概念

(1)区域层次

(2)位图填充

(3)区域公共属性

(4)通道显示属性

(5)区域反色

(6)区域QP保护

 

 

## 8.2.海思平台OSD理论学习2

### 8.2.1、海思平台OSD使用方法

(1)用户填充区域属性并创建区域

(2)将该区域指定到具体通道中(如 VENC)

(3)通过 HI_MPI_RGN_GetAttr、 HI_MPI_RGN_SetAttr 获取和设置区域属性

(4)通过 HI_MPI_RGN_SetBitMap(仅针对 Overlay)设置区域的位图信息

(5)通过 HI_MPI_RGN_GetDisplayAttr 和 HI_MPI_RGN_SetDisplayAttr 获取和设置区

域在某通道（如 VENC 通道）的通道显示属性。

(6)最后用户可以将该区域从通道中撤出（非必须操作），再销毁区域

### 8.2.2、海思平台OSD的API和关键数据结构

 

 

## 8.3.OSD实验演示和代码框架分析

### 8.3.1、OSD实验演示

### 8.3.2、OSD代码框架分析

 

 

## 8.4.OSD代码实现分析1

(1)函数调用关系

  SAMPLE_RGN_CreateVideoRegion

(2)RGN画布尺寸计算：以像素为单位。原始图像是bpp24的，每个像素3字节。而画布的图像是ARGB1555的，所以每个像素是2字节。所以画布每一行的像素数是图像宽度*3/2  

(3)整个图像的坐标系是左上角是(0,0)点，宽度方向是x，高度方向是y

 

 

## 8.5.OSD代码实现分析2-前景和背景透明

### 8.5.1、函数调用层次更新

  SAMPLE_RGN_CreateVideoRegion

​    SAMPLE_RGN_CreateOverlayForVenc

​      HI_MPI_RGN_Create

​      HI_MPI_RGN_AttachToChn

​    SAMPLE_RGN_Add

​      HI_MPI_RGN_GetAttr

​      HI_MPI_RGN_GetCanvasInfo

​      SAMPLE_RGN_UpdateCanvas

​       SAMPLE_RGN_CreateSurfaceByCanvas

​         SAMPLE_RGN_LoadCanvasEx

​           SAMPLE_RGN_LoadBMPCanvas_Logo

​             GetBmpInfo

​             malloc

​             fseek

​             fread

​             OSD_MAKECOLOR_U16      

​      HI_MPI_RGN_UpdateCanvas

​      

​    SAMPLE_RGN_CreateOverlayForVenc

​    SAMPLE_RGN_AddVideoTimestamp

### 8.5.2、RGN通道属性分析

### 8.5.3、通过实验来帮助理解代码和概念

(1)前景透明度

(2)背景透明度

(3)画布背景色

### 8.5.4、总结

(1)所谓前景foreground，就是图片中显示的内容部分；所谓背景background，就是图片中没有内容的部分。

(2)前景和背景的透明度范围都是0-128，其中0代表全透明，128代表全不透明

(3)前景和背景透明度可以同时设置，各自起作用，互不影响。

(4)stRgnAttr.unAttr.stOverlay.u32BgColor是RGN的画布（canvas，等同于LCD显示时的显存fb）的背景颜色。也就是画布中没有被填充的部分默认显示的颜色。

 

 

## 8.6.OSD代码实现分析3-RGN内容填充

BMP图片中存储图像的像素顺序，和RGN的canvas里像素顺序是不同的。

 

 

## 8.7.OSD代码实现分析4-RGN内容填充

 

 

## 8.8.OSD代码实现分析5-动态刷新BMP

总结：左下角的就是单bmp图片OSD，静态的；右下角是多BMP图片组合，且动态刷新显示。

 

 

## 8.9.字库字符实现OSD

### 8.9.1、代码框架分析

### 8.9.2、原理讲解

HH_OSD_Init     初始化这一套

  HH_OSD_SetOsdPosDefault

  HH_OSD_GetOrg

  HH_OSD_GetColor

  HH_OSD_GetTitle

  HI_Create_Osd

​    HI_OSD_Build

​      HI_OSD_Parse_OsdTitle

​      HI_OSD_Get_BmpSize

​      HI_Create_Osd_Reg

​       HI_MPI_RGN_Create

​       HI_MPI_RGN_AttachToChn

​      HI_OSD_Create_Bitmap

​       HI_OSD_CreateBMP      // 用字库来生成bmp文件的

​       HI_MPI_RGN_SetBitMap    // 真正设置osd显示的API

​      HI_OSD_Set_Show

​       HI_MPI_RGN_GetDisplayAttr

​       HI_MPI_RGN_SetDisplayAttr

HH_OSD_All_Refresh

  HH_OSD_AllTime_Refresh

​    HH_OSD_ChnTime_Refresh

​      HH_OSD_ChName_Refresh

​       HH_OSD_GetLogoHandle

​       HH_OSD_GetTitle

​       HI_OSD_Parse_OsdTitle

​       HI_OSD_Get_BmpSize

​       HI_OSD_Create_Bitmap

​         HI_OSD_CreateBMP

​         HI_MPI_RGN_SetBitMap

​       HH_OSD_Show_Refresh

​         HH_OSD_GetLogoHandle

​         HH_OSD_GetShow

​         HI_OSD_Set_Show

  HH_OSD_AllName_Refresh

 

 

总结：字库字符实现OSD的本质原理是：先由字库生成对应文字内容的bmp文件，然后将bmp文件显示在osd中。

 

 

## 8.10.字库字符实现OSD源码解析1

 

 

## 8.11.字库字符实现OSD源码解析2

 

 

## 8.12.字库字符实现OSD源码解析3

 

 

## 8.13.字库字符实现OSD源码解析4

#  